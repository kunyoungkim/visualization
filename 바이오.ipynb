{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvsuYMDchtvU",
        "outputId": "fd72b877-1e70-4426-b475-ca8aa4f34a04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.4.0-py3-none-any.whl (409 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.6/409.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.12.0-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.0/226.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.22)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
            "Installing collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.2.4 alembic-1.12.0 colorlog-6.7.0 optuna-3.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-pTN3bWhFcOx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ignore warning\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ML\n",
        "from sklearn.ensemble import RandomForestClassifier  # Bagging\n",
        "from sklearn.linear_model import LogisticRegression  # LogisticRegression\n",
        "# from sklearn.svm import SVC                          # SVM\n",
        "\n",
        "from xgboost.sklearn import XGBClassifier            # GBM\n",
        "from lightgbm.sklearn import LGBMClassifier          # LGBM\n",
        "\n",
        "# train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# DL\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, ReLU, Softmax, Dropout\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "# for checking multi-collinearity\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# KFold(CV), partial : for optuna\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import KNNImputer\n",
        "from functools import partial\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# 피처 스케일링\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# AutoML framework\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uHLOzDPTde7N"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "seed_everything()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gg4caqUTVARl"
      },
      "outputs": [],
      "source": [
        "is_IQR = False\n",
        "is_scaler = False\n",
        "is_dropcol = True\n",
        "is_012 = False\n",
        "# ======= Set K of K-fold =======\n",
        "K = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBpo6XQ-FnQ1"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJDCF-zCFoAo"
      },
      "outputs": [],
      "source": [
        "base_path = '/content/drive/MyDrive/Colab Notebooks/크몽/바이오/'\n",
        "\n",
        "df_tc = pd.read_csv(base_path + '(20230923)+Test+cohort+(all+biomarkers)_췌장암.csv')\n",
        "df_td = pd.read_csv(base_path + '(20230923)+Test+cohort+(all+biomarkers)_대조군.csv')\n",
        "df_vc = pd.read_csv(base_path + '(20230923)+Validation+cohort+(all+biomarkers)_췌장암.csv')\n",
        "df_vd = pd.read_csv(base_path + '(20230923)+Validation+cohort+(all+biomarkers)_대조군.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chsmESePRcHb"
      },
      "outputs": [],
      "source": [
        "# 컬럼 이름 변경\n",
        "import re\n",
        "\n",
        "# Function to clean the column names\n",
        "def clean_column_name(col):\n",
        "    if col == 'Stage(TNM)':  # Skip this specific column\n",
        "      return col\n",
        "    col = re.sub(r'\\([^)]*\\)', '', col)  # Remove content within parentheses\n",
        "    col = re.sub(r'^\\d+-', '', col)  # Remove leading numbers and dash\n",
        "    col = col.replace('\\n', '')  # Remove newline characters\n",
        "    return col.strip()\n",
        "\n",
        "# # 기존 컬럼 리스트\n",
        "original_columns1 = df_tc.columns.tolist()\n",
        "original_columns2 = df_td.columns.tolist()\n",
        "\n",
        "# Clean the column names\n",
        "new_columns1 = [clean_column_name(col) for col in original_columns1]\n",
        "new_columns2 = [clean_column_name(col) for col in original_columns2]\n",
        "\n",
        "df_tc.columns = new_columns1\n",
        "df_td.columns = new_columns2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9E7MtfYcGGsM"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZVllRvgj6KZ"
      },
      "source": [
        "## Knn imputer (결측치 채우기)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gXLsBd5ILz2"
      },
      "outputs": [],
      "source": [
        "imputer = KNNImputer(n_neighbors=5)\n",
        "\n",
        "df_tc_Stage = df_tc.pop('Stage(TNM)')\n",
        "\n",
        "imputed_data = imputer.fit_transform(df_tc)\n",
        "imputed_df = pd.DataFrame(imputed_data, columns=df_tc.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVHzCkv6JgXb"
      },
      "outputs": [],
      "source": [
        "imputer2 = KNNImputer(n_neighbors=5)\n",
        "imputed_data2 = imputer2.fit_transform(df_td)\n",
        "imputed_df2 = pd.DataFrame(imputed_data2, columns=df_td.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZPZSZwjI3uo"
      },
      "outputs": [],
      "source": [
        "df_tc = imputed_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfw61YC8NK8_"
      },
      "outputs": [],
      "source": [
        "df_td = imputed_df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jQUzo-rmj5P"
      },
      "outputs": [],
      "source": [
        "print(df_td.shape)\n",
        "print(df_tc.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cqJdCg4G31m"
      },
      "source": [
        "## 컬럼 제거\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2seypOtNtGD"
      },
      "outputs": [],
      "source": [
        "# No. 컬럼 제거\n",
        "df_tc = df_tc.drop(columns='No.')\n",
        "df_td = df_td.drop(columns='No.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3uJM1tXHVh2"
      },
      "source": [
        "#### 췌장암"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4airwT6GWRu"
      },
      "outputs": [],
      "source": [
        "if is_dropcol:\n",
        "  # df_tc = df_tc.drop(columns=['TSP-2',\n",
        "  #                                   'G-CSF',\n",
        "  #                                   'FGF-1',\n",
        "  #                                   'MIF',\n",
        "  #                                   'IL-6',\n",
        "  #                                   'TNFa',\n",
        "  #                                   'Cyfra21-1',\n",
        "  #                                   'FGF2',\n",
        "  #                                   'bHCG',\n",
        "  #                                   'HE4',\n",
        "  #                                   'TGFa'])\n",
        "\n",
        "\n",
        "  df_tc_drop = df_tc[['CA19-9','IL-8','GDF15']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZtVj_kJHYJF"
      },
      "source": [
        "#### 대조군"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlSKDo_EHTDz"
      },
      "outputs": [],
      "source": [
        "if is_dropcol:\n",
        "  # df_td = df_td.drop(columns=['TSP-2',\n",
        "  #                                   'G-CSF',\n",
        "  #                                   'FGF-1',\n",
        "  #                                   'MIF',\n",
        "  #                                   'IL-6',\n",
        "  #                                   'TNFa',\n",
        "  #                                   'Cyfra21-1',\n",
        "  #                                   'FGF2',\n",
        "  #                                   'bHCG',\n",
        "  #                                   'HE4',\n",
        "  #                                   'TGFa'])\n",
        "\n",
        "\n",
        "  df_td_drop = df_td[['CA19-9','IL-8','GDF15']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_nwUct3NCj-"
      },
      "source": [
        "### histplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCHAmUuxMH1K"
      },
      "outputs": [],
      "source": [
        "# for i in df_td.columns:\n",
        "#   plt.figure(figsize=(12,6))\n",
        "#   sns.histplot(df_td[i],bins=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYJilRwpMUtB"
      },
      "outputs": [],
      "source": [
        "# for i in df_tc.columns:\n",
        "#   plt.figure(figsize=(12,6))\n",
        "#   sns.histplot(df_tc[i],bins=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R50YuihrNcb_"
      },
      "source": [
        "## 이상치 제거(IQR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mKJedYEOjKA"
      },
      "outputs": [],
      "source": [
        "print(df_td.shape)\n",
        "print(df_tc.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqQ76uCGNedX"
      },
      "outputs": [],
      "source": [
        "if is_IQR:\n",
        "  # IQR 계산\n",
        "  for i in df_tc.columns:\n",
        "    Q1 = df_tc[i].quantile(0.25)\n",
        "    Q3 = df_tc[i].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # 이상치 임계값 설정 (일반적으로 IQR * 1.5 사용)\n",
        "    threshold = 4 * IQR\n",
        "\n",
        "    # 이상치 식별\n",
        "    outliers = (df_tc[i] < Q1 - threshold) | (df_tc[i] > Q3 + threshold)\n",
        "\n",
        "    # 이상치 제거\n",
        "    df_tc = df_tc[~outliers]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Voq83uatPXfo"
      },
      "outputs": [],
      "source": [
        "if is_IQR:\n",
        "  # IQR 계산\n",
        "  for i in df_td.columns:\n",
        "    Q1 = df_td[i].quantile(0.25)\n",
        "    Q3 = df_td[i].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # 이상치 임계값 설정 (일반적으로 IQR * 1.5 사용)\n",
        "    threshold = 4 * IQR\n",
        "\n",
        "    # 이상치 식별\n",
        "    outliers = (df_td[i] < Q1 - threshold) | (df_td[i] > Q3 + threshold)\n",
        "\n",
        "    # 이상치 제거\n",
        "    df_td = df_td[~outliers]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YycTdgBpPsJF"
      },
      "outputs": [],
      "source": [
        "print(df_td.shape)\n",
        "print(df_tc.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugLcfsa3TzZP"
      },
      "source": [
        "## 표준화(데이터 합쳐서 진행)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEE9wdoXUFlz"
      },
      "outputs": [],
      "source": [
        "# 데이터셋 합치기\n",
        "df_t = pd.concat([df_tc, df_td], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeva17JvT1YP"
      },
      "outputs": [],
      "source": [
        "# 표준화\n",
        "scaler = StandardScaler()\n",
        "\n",
        "df_ts = scaler.fit_transform(df_t)\n",
        "df_ts = pd.DataFrame(df_ts, columns=df_t.columns)\n",
        "\n",
        "# 표준화 시킨거에 Stage(TNM) 칼럼 다시 합치기\n",
        "df_ts['Stage(TNM)'] = df_tc_Stage\n",
        "df_ts['Stage(TNM)'] = df_ts['Stage(TNM)'].fillna(0)\n",
        "\n",
        "df_ts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9bFgS-nWsL6"
      },
      "outputs": [],
      "source": [
        "df_t['Stage(TNM)'] = df_tc_Stage\n",
        "df_t['Stage(TNM)'] = df_t['Stage(TNM)'].fillna(0)\n",
        "\n",
        "if is_dropcol:\n",
        "  # 데이터셋 합치기\n",
        "  df_t_drop = pd.concat([df_tc_drop, df_td_drop], ignore_index=True)\n",
        "  df_t_drop['Stage(TNM)'] = df_tc_Stage\n",
        "  df_t_drop['Stage(TNM)'] = df_t_drop['Stage(TNM)'].fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # CSV 파일로 저장\n",
        "# df_t.to_csv(base_path + \"concat_data.csv\", index=False)"
      ],
      "metadata": {
        "id": "GHv7aQiV65xP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no4TkrYDHoes"
      },
      "source": [
        "### 01로 변경"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKxz1Y8mIU5-"
      },
      "outputs": [],
      "source": [
        "df_t01 = df_t.copy()\n",
        "df_t01.loc[df_t01['Stage(TNM)'] >= 1, 'Stage(TNM)'] = 1\n",
        "\n",
        "df_ts01 = df_ts.copy()\n",
        "df_ts01.loc[df_ts01['Stage(TNM)'] >= 1, 'Stage(TNM)'] = 1\n",
        "\n",
        "if is_dropcol:\n",
        "  df_t01_drop = df_t_drop.copy()\n",
        "  df_t01_drop.loc[df_t01_drop['Stage(TNM)'] >= 1, 'Stage(TNM)'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXluZmXSsDO3"
      },
      "outputs": [],
      "source": [
        "# 'Stage(TNM)' 열을 기준으로 데이터를 분할\n",
        "df_t0 = df_t01[df_t01['Stage(TNM)'] == 0]\n",
        "df_t1 = df_t01[df_t01['Stage(TNM)'] == 1]\n",
        "\n",
        "# 'Stage(TNM)' 열을 기준으로 데이터를 분할\n",
        "df_ts0 = df_ts01[df_ts01['Stage(TNM)'] == 0]\n",
        "df_ts1 = df_ts01[df_ts01['Stage(TNM)'] == 1]\n",
        "\n",
        "if is_dropcol:\n",
        "  df_t0_drop = df_t01_drop[df_t01_drop['Stage(TNM)'] == 0]\n",
        "  df_t1_drop = df_t01_drop[df_t01_drop['Stage(TNM)'] == 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3NRzK38h4kA"
      },
      "outputs": [],
      "source": [
        "df_t01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTyRc-ViqC9w"
      },
      "source": [
        "### 012로 변경"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0s5lC8SkqF5g"
      },
      "outputs": [],
      "source": [
        "df_t00 = df_t[df_t['Stage(TNM)'] == 0]\n",
        "df_t12 = df_t[(df_t['Stage(TNM)'] == 1) | (df_t['Stage(TNM)'] == 2)]\n",
        "df_t34 = df_t[(df_t['Stage(TNM)'] == 3) | (df_t['Stage(TNM)'] == 4)]\n",
        "\n",
        "df_ts00 = df_ts[df_ts['Stage(TNM)'] == 0]\n",
        "df_ts12 = df_ts[(df_ts['Stage(TNM)'] == 1) | (df_ts['Stage(TNM)'] == 2)]\n",
        "df_ts34 = df_ts[(df_ts['Stage(TNM)'] == 3) | (df_ts['Stage(TNM)'] == 4)]\n",
        "\n",
        "if is_dropcol:\n",
        "  df_t00_drop = df_t_drop[df_t_drop['Stage(TNM)'] == 0]\n",
        "  df_t12_drop = df_t_drop[(df_t_drop['Stage(TNM)'] == 1) | (df_t_drop['Stage(TNM)'] == 2)]\n",
        "  df_t34_drop = df_t_drop[(df_t_drop['Stage(TNM)'] == 3) | (df_t_drop['Stage(TNM)'] == 4)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDFR1P1IRuef"
      },
      "source": [
        "# 히트맵"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rDnIKAUcw-f"
      },
      "outputs": [],
      "source": [
        "sns.clustermap(data=df_ts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWw7L_f2e1bZ"
      },
      "outputs": [],
      "source": [
        "sns.clustermap(data=df_t, z_score=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLmwT3LWRwOe"
      },
      "outputs": [],
      "source": [
        "lut = dict(zip(df_ts['Stage(TNM)'].unique(), \"rgb\"))\n",
        "\n",
        "row_colors = df_ts['Stage(TNM)'].map(lut)\n",
        "\n",
        "# 데이터를 전치하여 가로축과 세로축을 바꿈\n",
        "df_trans = df_ts.transpose()\n",
        "\n",
        "# sns.clustermap(df_ts, row_colors=row_colors)\n",
        "\n",
        "# clustermap 그리기 (전치된 데이터 사용)\n",
        "sns.clustermap(df_trans, row_cluster=False, col_cluster=False, row_colors=row_colors, figsize=(6, 6))\n",
        "\n",
        "# 그래프 출력\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ry0XXqBFYL9D"
      },
      "outputs": [],
      "source": [
        "lut = dict(zip(df_ts['Stage(TNM)'].unique(), \"rgb\"))\n",
        "\n",
        "row_colors = df_ts['Stage(TNM)'].map(lut)\n",
        "\n",
        "# sns.clustermap(df_ts, row_colors=row_colors)\n",
        "\n",
        "# clustermap 그리기 (row_colors로 행을 정렬)\n",
        "sns.clustermap(df_ts.drop(columns=['Stage(TNM)']).T, col_colors=row_colors, row_cluster=True, col_cluster=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Io2U65NCiG1Z"
      },
      "outputs": [],
      "source": [
        "lut = dict(zip(df_ts01['Stage(TNM)'].unique(), \"rgb\"))\n",
        "\n",
        "row_colors = df_ts01['Stage(TNM)'].map(lut)\n",
        "\n",
        "# sns.clustermap(df_ts, row_colors=row_colors)\n",
        "\n",
        "# clustermap 그리기 (row_colors로 행을 정렬)\n",
        "sns.clustermap(df_ts01.drop(columns=['Stage(TNM)']).T, col_colors=row_colors, row_cluster=True, col_cluster=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJ90oRyVhNGH"
      },
      "outputs": [],
      "source": [
        "# 색상 매핑 딕셔너리 생성\n",
        "# 색상 매핑 딕셔너리 생성\n",
        "lut = {0: \"r\", 1: \"g\"}\n",
        "\n",
        "col_colors0 = df_ts0['Stage(TNM)'].map(lut)\n",
        "col_colors1 = df_ts1['Stage(TNM)'].map(lut)\n",
        "\n",
        "# 각각의 데이터프레임에 대해 clustermap 그리기\n",
        "g_0 = sns.clustermap(df_ts0.T, col_colors=col_colors0, row_cluster=False, col_cluster=False)\n",
        "g_1 = sns.clustermap(df_ts1.T, col_colors=col_colors1, row_cluster=True, col_cluster=False)\n",
        "\n",
        "# 그래프 출력\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oE8cRGSjVLf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "# 각각의 데이터프레임에 대해 clustermap 그리기\n",
        "g_0 = sns.clustermap(df_t0.T, col_colors=col_colors0, row_cluster=True, col_cluster=False)\n",
        "g_1 = sns.clustermap(df_t1.T, col_colors=col_colors1, row_cluster=True, col_cluster=False)\n",
        "\n",
        "# 그래프를 하나의 figure에 합치기\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "gs = gridspec.GridSpec(1, 2, width_ratios=[1, 1])\n",
        "ax0 = plt.subplot(gs[0])\n",
        "ax1 = plt.subplot(gs[1])\n",
        "\n",
        "# 각각의 그래프를 figure에 추가\n",
        "ax0.imshow(g_0.data2d, aspect='auto', cmap='coolwarm', origin='lower')\n",
        "ax1.imshow(g_1.data2d, aspect='auto', cmap='coolwarm', origin='lower')\n",
        "\n",
        "# 각각의 그래프에 타이틀 추가\n",
        "ax0.set_title('Clustermap 1')\n",
        "ax1.set_title('Clustermap 2')\n",
        "\n",
        "# 축 숨기기\n",
        "ax0.axis('off')\n",
        "ax1.axis('off')\n",
        "\n",
        "# 그래프 출력\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sV0UUzh8ngqV"
      },
      "outputs": [],
      "source": [
        "import matplotlib.gridspec as gridspec\n",
        "# 그래프를 하나의 figure에 합치기\n",
        "fig = plt.figure(figsize=(20, 11))\n",
        "gs = gridspec.GridSpec(1, 2, width_ratios=[1, 1])\n",
        "ax0 = plt.subplot(gs[0])\n",
        "ax1 = plt.subplot(gs[1])\n",
        "\n",
        "# 'Stage(TNM)' 열을 기준으로 데이터를 분할\n",
        "df_0 = df_ts01[df_ts01['Stage(TNM)'] == 0]\n",
        "df_1 = df_ts01[df_ts01['Stage(TNM)'] == 1]\n",
        "\n",
        "# 색상 매핑 딕셔너리 생성\n",
        "lut = {0: \"r\", 1: \"g\"}\n",
        "\n",
        "col_colors0 = df_ts0['Stage(TNM)'].map(lut)\n",
        "col_colors1 = df_ts1['Stage(TNM)'].map(lut)\n",
        "\n",
        "# 각각의 데이터프레임에 대해 clustermap 그리기\n",
        "g_0 = sns.clustermap(df_ts0.T, col_colors=col_colors0, row_cluster=True, col_cluster=False)\n",
        "g_1 = sns.clustermap(df_ts1.T, col_colors=col_colors1, row_cluster=True, col_cluster=False)\n",
        "\n",
        "\n",
        "# # 각각의 그래프를 figure에 추가\n",
        "ax0.imshow(g_0.data2d,aspect='auto',cmap='RdBu', origin='lower')\n",
        "ax1.imshow(g_1.data2d,aspect='auto',cmap='RdBu', origin='lower')\n",
        "\n",
        "\n",
        "# # 각각의 그래프에 타이틀 추가\n",
        "# ax0.set_title('Clustermap 1')\n",
        "# ax1.set_title('Clustermap 2')\n",
        "\n",
        "# # 축 숨기기\n",
        "# ax0.axis('off')\n",
        "ax1.axis('off')\n",
        "\n",
        "# # 그래프 출력\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pioyXnwBoHgk"
      },
      "outputs": [],
      "source": [
        "from matplotlib import gridspec\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
        "\n",
        "# Create a new figure with constrained layout\n",
        "fig = plt.figure(figsize=(20, 15), constrained_layout=True)\n",
        "\n",
        "# Create a gridspec\n",
        "gs = gridspec.GridSpec(1, 2, width_ratios=[1, 1], wspace=0.03)  # wspace is the width space\n",
        "\n",
        "# Create separate clustermaps\n",
        "g_0 = sns.clustermap(df_ts0.T, col_colors=col_colors0, row_cluster=False, col_cluster=False, figsize=(10, 15), cbar_pos=None)\n",
        "plt.close()  # Close the plot to prevent it from displaying\n",
        "g_1 = sns.clustermap(df_ts1.T, col_colors=col_colors1, row_cluster=False, col_cluster=False, figsize=(10, 15), cbar_pos=None)\n",
        "plt.close()  # Close the plot to prevent it from displaying\n",
        "\n",
        "# Create a new axes\n",
        "ax0 = fig.add_subplot(gs[0])\n",
        "ax1 = fig.add_subplot(gs[1])\n",
        "\n",
        "# Draw the clustermaps onto the new axes\n",
        "canvas0 = FigureCanvas(g_0.fig)\n",
        "canvas0.draw()\n",
        "ax0.imshow(canvas0.buffer_rgba())\n",
        "\n",
        "canvas1 = FigureCanvas(g_1.fig)\n",
        "canvas1.draw()\n",
        "ax1.imshow(canvas1.buffer_rgba())\n",
        "\n",
        "# Remove x and y ticks from the new axes\n",
        "ax0.set_xticks([])\n",
        "ax0.set_yticks([])\n",
        "ax1.set_xticks([])\n",
        "ax1.set_yticks([])\n",
        "\n",
        "# Show the final plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uX3Tcnjoq_iA"
      },
      "outputs": [],
      "source": [
        "# Create a figure to contain the subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 15), gridspec_kw={'width_ratios': [1, 1], 'wspace': 0.003})\n",
        "\n",
        "# Create the first heatmap\n",
        "sns.heatmap(df_t0.T, ax=ax1, cmap=\"RdBu\", cbar=False)\n",
        "\n",
        "# Add color bar to separate samples based on Stage(TNM)\n",
        "ax1.add_patch(plt.Rectangle((0, 0), df_t0.shape[0], 1, edgecolor=\"red\", facecolor=\"red\", lw=0))\n",
        "\n",
        "# Create the second heatmap\n",
        "sns.heatmap(df_t1.T, ax=ax2, cmap=\"RdBu\", cbar=True, yticklabels=False)\n",
        "\n",
        "# Add color bar to separate samples based on Stage(TNM)\n",
        "ax2.add_patch(plt.Rectangle((0, 0), df_t1.shape[0], 1, edgecolor=\"green\", facecolor=\"green\", lw=0))\n",
        "\n",
        "# Add column names to the left of the first heatmap\n",
        "ax1.set_yticklabels(df_0.columns, rotation=0)\n",
        "\n",
        "# Hide the axes\n",
        "ax2.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwH-mkv-rgLF"
      },
      "outputs": [],
      "source": [
        "# Create a figure to contain the subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 15), gridspec_kw={'width_ratios': [1, 1], 'wspace': 0.01})\n",
        "\n",
        "# Create the first heatmap\n",
        "sns.heatmap(df_t0.T, ax=ax1, cmap=\"coolwarm\", cbar=False)\n",
        "\n",
        "# Add color bar to separate samples based on Stage(TNM)\n",
        "ax1.add_patch(plt.Rectangle((0, 0), df_t0.shape[0], 1, edgecolor=\"red\", facecolor=\"red\", lw=0))\n",
        "\n",
        "# Create the second heatmap\n",
        "sns.heatmap(df_1.T, ax=ax2, cmap=\"coolwarm\", cbar=True, yticklabels=False)\n",
        "\n",
        "# Add color bar to separate samples based on Stage(TNM)\n",
        "ax2.add_patch(plt.Rectangle((0, 0), df_t1.shape[0], 1, edgecolor=\"green\", facecolor=\"green\", lw=0))\n",
        "\n",
        "# Add column names to the left of the first heatmap\n",
        "ax1.set_yticklabels(df_t0.columns, rotation=0)\n",
        "\n",
        "# Remove x-axis labels and ticks from the first heatmap\n",
        "ax1.set_xticks([])\n",
        "ax1.set_xticklabels([])\n",
        "ax1.set_xlabel(\"\")\n",
        "\n",
        "# Hide the axes of the second heatmap\n",
        "ax2.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MseQDRQsak_"
      },
      "outputs": [],
      "source": [
        "# Create a figure to contain the subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 15), gridspec_kw={'width_ratios': [1, 1], 'wspace': 0.005})\n",
        "\n",
        "# Create the first heatmap\n",
        "sns.heatmap(df_t0.T, ax=ax1, cmap=\"coolwarm\", cbar=False, vmin = -2.5, vmax=3)\n",
        "\n",
        "# Create the second heatmap\n",
        "sns.heatmap(df_t1.T, ax=ax2, cmap=\"coolwarm\", cbar=True, yticklabels=False, vmin = -2.5, vmax=3)\n",
        "\n",
        "# Add column names to the left of the first heatmap\n",
        "ax1.set_yticklabels(df_t0.columns, rotation=0)\n",
        "\n",
        "# Remove x-axis labels and ticks from the first heatmap\n",
        "ax1.set_xticks([])\n",
        "ax1.set_xticklabels([])\n",
        "ax1.set_xlabel(\"\")\n",
        "\n",
        "# Hide the axes of the second heatmap\n",
        "ax2.axis('off')\n",
        "\n",
        "# Add a color bar above the heatmaps to separate samples based on Stage(TNM)\n",
        "colorbar_ax1 = fig.add_axes([ax1.get_position().x0, ax1.get_position().y1 + 0.005, ax1.get_position().width, 0.02])\n",
        "colorbar_ax2 = fig.add_axes([ax2.get_position().x0, ax2.get_position().y1 + 0.005, ax2.get_position().width, 0.02])\n",
        "\n",
        "colorbar_ax1.text(-0.13, 0.5, 'Stage(TNM)', verticalalignment='center', horizontalalignment='left', fontsize=12, va='center')\n",
        "colorbar_ax1.add_patch(plt.Rectangle((0, 0), 1, 1, facecolor=\"red\", edgecolor=\"red\"))\n",
        "colorbar_ax1.axis('off')\n",
        "\n",
        "colorbar_ax2.add_patch(plt.Rectangle((0, 0), 1, 1, facecolor=\"green\", edgecolor=\"green\"))\n",
        "colorbar_ax2.axis('off')\n",
        "\n",
        "\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFAunwLAt8fg"
      },
      "outputs": [],
      "source": [
        "# Create a figure to contain the subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 15), gridspec_kw={'width_ratios': [1, 1], 'wspace': 0.005})\n",
        "\n",
        "# Verify if 'Stage(TNM)' exists in the DataFrame, if so, drop it\n",
        "if 'Stage(TNM)' in df_t0.columns:\n",
        "    df_0 = df_t0.drop(columns=['Stage(TNM)'])\n",
        "if 'Stage(TNM)' in df_t1.columns:\n",
        "    df_1 = df_t1.drop(columns=['Stage(TNM)'])\n",
        "\n",
        "# Create the first heatmap\n",
        "sns.heatmap(df_0.T, ax=ax1, cmap=\"coolwarm\", cbar=False, vmin = -2, vmax=2)\n",
        "\n",
        "# Create the second heatmap\n",
        "sns.heatmap(df_1.T, ax=ax2, cmap=\"coolwarm\", cbar=True, yticklabels=False, vmin = -2, vmax=2)\n",
        "\n",
        "# Add column names to the left of the first heatmap\n",
        "ax1.set_yticklabels(df_0.columns, rotation=0)\n",
        "\n",
        "# Remove x-axis labels and ticks from the first heatmap\n",
        "ax1.set_xticks([])\n",
        "ax1.set_xticklabels([])\n",
        "ax1.set_xlabel(\"\")\n",
        "\n",
        "# Hide the axes of the second heatmap\n",
        "ax2.axis('off')\n",
        "\n",
        "# Add a color bar above the heatmaps to separate samples based on Stage(TNM)\n",
        "colorbar_ax1 = fig.add_axes([ax1.get_position().x0, ax1.get_position().y1 + 0.005, ax1.get_position().width, 0.02])\n",
        "colorbar_ax2 = fig.add_axes([ax2.get_position().x0, ax2.get_position().y1 + 0.005, ax2.get_position().width, 0.02])\n",
        "\n",
        "colorbar_ax1.text(-0.13, 0.5, 'Stage(TNM)', verticalalignment='center', horizontalalignment='left', fontsize=12, va='center')\n",
        "colorbar_ax1.add_patch(plt.Rectangle((0, 0), 1, 1, facecolor=\"red\", edgecolor=\"red\"))\n",
        "colorbar_ax1.axis('off')\n",
        "\n",
        "colorbar_ax2.add_patch(plt.Rectangle((0, 0), 1, 1, facecolor=\"green\", edgecolor=\"green\"))\n",
        "colorbar_ax2.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7OhF4FSxuAy"
      },
      "outputs": [],
      "source": [
        "# Create a figure to contain the subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 15), gridspec_kw={'width_ratios': [1, 1], 'wspace': 0.005})\n",
        "\n",
        "# Create the first heatmap\n",
        "cax1 = sns.heatmap(df_t0.T, ax=ax1, cmap=\"coolwarm\", cbar=False, vmin = -2.5, vmax=3)\n",
        "cax1.add_patch(plt.Rectangle((0, 0), df_t0.shape[0], len(df_t0.columns), fill=False, edgecolor=\"black\", lw=2))\n",
        "\n",
        "# Create the second heatmap\n",
        "cax2 = sns.heatmap(df_t1.T, ax=ax2, cmap=\"coolwarm\", cbar=False, yticklabels=False, vmin = -2.5, vmax=3)\n",
        "cax2.add_patch(plt.Rectangle((0, 0), df_t1.shape[0], len(df_t1.columns), fill=False, edgecolor=\"black\", lw=2))\n",
        "\n",
        "# Add column names to the left of the first heatmap\n",
        "ax1.set_yticklabels(df_t0.columns, rotation=0)\n",
        "\n",
        "# Remove x-axis labels and ticks from the first heatmap\n",
        "ax1.set_xticks([])\n",
        "ax1.set_xticklabels([])\n",
        "ax1.set_xlabel(\"\")\n",
        "\n",
        "# Hide the axes of the second heatmap\n",
        "ax2.axis('off')\n",
        "\n",
        "# Add a color bar above the heatmaps to separate samples based on Stage(TNM)\n",
        "colorbar_ax1 = fig.add_axes([ax1.get_position().x0, ax1.get_position().y1 + 0.005, ax1.get_position().width, 0.02])\n",
        "colorbar_ax2 = fig.add_axes([ax2.get_position().x0, ax2.get_position().y1 + 0.005, ax2.get_position().width, 0.02])\n",
        "\n",
        "# Add black border around color bars\n",
        "colorbar_ax1.add_patch(plt.Rectangle((-1, -1), 2, 2, facecolor=\"none\", edgecolor=\"black\", linewidth=2))\n",
        "colorbar_ax2.add_patch(plt.Rectangle((-1, -1), 2, 2, facecolor=\"none\", edgecolor=\"black\", linewidth=2))\n",
        "\n",
        "colorbar_ax1.text(-0.13, 0.5, 'Stage(TNM)', verticalalignment='center', horizontalalignment='left', fontsize=12, va='center')\n",
        "colorbar_ax1.add_patch(plt.Rectangle((0, 0), 1, 1, facecolor=\"red\", edgecolor=\"red\"))\n",
        "colorbar_ax1.axis('off')\n",
        "\n",
        "colorbar_ax2.add_patch(plt.Rectangle((0, 0), 1, 1, facecolor=\"green\", edgecolor=\"green\"))\n",
        "colorbar_ax2.axis('off')\n",
        "\n",
        "# Add a separate color bar (cbar) for the z-score\n",
        "cbar_ax = fig.add_axes([0.92, 0.4, 0.02, 0.2])\n",
        "cbar = plt.colorbar(ax2.collections[0], cax=cbar_ax)\n",
        "cbar_ax.set_title('z-score', pad=10, fontsize=14)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctRU6ZztxwCe"
      },
      "outputs": [],
      "source": [
        "from matplotlib.patches import Patch\n",
        "\n",
        "# Create a figure to contain the subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 15), gridspec_kw={'width_ratios': [1, 1], 'wspace': 0.007})\n",
        "\n",
        "# Create the first heatmap\n",
        "cax1 = sns.heatmap(df_t0.T, ax=ax1, cmap=\"coolwarm\", cbar=False, vmin = -2, vmax=2)\n",
        "cax1.add_patch(plt.Rectangle((0, 0), df_t0.shape[0], len(df_t0.columns), fill=False, edgecolor=\"black\", lw=3))\n",
        "\n",
        "# Create the second heatmap\n",
        "cax2 = sns.heatmap(df_t1.T, ax=ax2, cmap=\"coolwarm\", cbar=False, yticklabels=False, vmin = -2, vmax=2)\n",
        "cax2.add_patch(plt.Rectangle((0, 0), df_t1.shape[0], len(df_t1.columns), fill=False, edgecolor=\"black\", lw=3))\n",
        "\n",
        "# Add column names to the left of the first heatmap\n",
        "ax1.set_yticklabels(df_t0.columns, rotation=0)\n",
        "\n",
        "# Remove x-axis labels and ticks from the first heatmap\n",
        "ax1.set_xticks([])\n",
        "ax1.set_xticklabels([])\n",
        "ax1.set_xlabel(\"\")\n",
        "\n",
        "# Hide the axes of the second heatmap\n",
        "ax2.axis('off')\n",
        "\n",
        "# Add a color bar above the heatmaps to separate samples based on Stage(TNM)\n",
        "colorbar_ax1 = fig.add_axes([ax1.get_position().x0, ax1.get_position().y1 + 0.005, ax1.get_position().width, 0.025])\n",
        "colorbar_ax2 = fig.add_axes([ax2.get_position().x0, ax2.get_position().y1 + 0.005, ax2.get_position().width, 0.025])\n",
        "\n",
        "colorbar_ax1.text(-0.075, 0.5, 'Group', verticalalignment='center', horizontalalignment='left', fontsize=12, va='center')\n",
        "colorbar_ax1.add_patch(plt.Rectangle((0, 0), 1, 1, facecolor=\"#B22222\", edgecolor=\"black\", lw=3))\n",
        "colorbar_ax1.axis('off')\n",
        "\n",
        "colorbar_ax2.add_patch(plt.Rectangle((0, 0), 1, 1, facecolor=\"#1874CD\", edgecolor=\"black\", lw=3))\n",
        "colorbar_ax2.axis('off')\n",
        "\n",
        "# Add a separate color bar (cbar) for the z-score\n",
        "cbar_ax = fig.add_axes([0.92, 0.4, 0.02, 0.2])\n",
        "cbar = plt.colorbar(ax2.collections[0], cax=cbar_ax)\n",
        "cbar_ax.set_title('z-score', pad=10, fontsize=14)\n",
        "\n",
        "# Add a legend for Stage(TNM)\n",
        "legend_labels = [Patch(facecolor=\"#B22222\", edgecolor=\"black\", label='PDA'),\n",
        "                 Patch(facecolor=\"#1874CD\", edgecolor=\"black\", label='HC')]\n",
        "fig.legend(handles=legend_labels, loc='upper right', fontsize=12, frameon=False)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuxwLI7XzigS"
      },
      "outputs": [],
      "source": [
        "# # Create a figure to contain the subplots\n",
        "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 15), gridspec_kw={'width_ratios': [1, 1], 'wspace': 0.007})\n",
        "\n",
        "# # Verify if 'Stage(TNM)' exists in the DataFrame, if so, drop it\n",
        "# if 'Stage(TNM)' in df_ts0.columns:\n",
        "#     df_ts0 = df_ts0.drop(columns=['Stage(TNM)'])\n",
        "# if 'Stage(TNM)' in df_ts1.columns:\n",
        "#     df_ts1 = df_ts1.drop(columns=['Stage(TNM)'])\n",
        "\n",
        "# # Create the first heatmap\n",
        "# cax1 = sns.heatmap(df_ts0.T, ax=ax1, cmap=\"coolwarm\", cbar=False, vmin = -2, vmax=2)\n",
        "# cax1.add_patch(plt.Rectangle((0, 0), df_ts.shape[0], len(df_ts.columns), fill=False, edgecolor=\"black\", lw=3))\n",
        "\n",
        "# # Create the second heatmap\n",
        "# cax2 = sns.heatmap(df_ts1.T, ax=ax2, cmap=\"coolwarm\", cbar=False, yticklabels=False, vmin = -2, vmax=2)\n",
        "# cax2.add_patch(plt.Rectangle((0, 0), df_ts1.shape[0], len(df_ts.columns), fill=False, edgecolor=\"black\", lw=3))\n",
        "\n",
        "# # Add column names to the left of the first heatmap\n",
        "# ax1.set_yticklabels(df_ts.columns, rotation=0)\n",
        "\n",
        "# # Remove x-axis labels and ticks from the first heatmap\n",
        "# ax1.set_xticks([])\n",
        "# ax1.set_xticklabels([])\n",
        "# ax1.set_xlabel(\"\")\n",
        "\n",
        "# # Hide the axes of the second heatmap\n",
        "# ax2.axis('off')\n",
        "\n",
        "# # Add a color bar above the heatmaps to separate samples based on Stage(TNM)\n",
        "# colorbar_ax1 = fig.add_axes([ax1.get_position().x0, ax1.get_position().y1 + 0.005, ax1.get_position().width, 0.025])\n",
        "# colorbar_ax2 = fig.add_axes([ax2.get_position().x0, ax2.get_position().y1 + 0.005, ax2.get_position().width, 0.025])\n",
        "\n",
        "# colorbar_ax1.text(-0.085, 0.5, 'Group', verticalalignment='center', horizontalalignment='left', fontsize=14, va='center')\n",
        "# colorbar_ax1.add_patch(plt.Rectangle((0, 0), 1, 1, facecolor=\"#B22222\", edgecolor=\"black\", lw=3))\n",
        "# colorbar_ax1.axis('off')\n",
        "\n",
        "# colorbar_ax2.add_patch(plt.Rectangle((0, 0), 1, 1, facecolor=\"#1874CD\", edgecolor=\"black\", lw=3))\n",
        "# colorbar_ax2.axis('off')\n",
        "\n",
        "# # Add a separate color bar (cbar) for the z-score\n",
        "# cbar_ax = fig.add_axes([0.92, 0.6, 0.015, 0.1])\n",
        "# cbar = plt.colorbar(ax2.collections[0], cax=cbar_ax)\n",
        "# cbar_ax.set_title('Z-score', pad=10, fontsize=15, fontweight='bold')\n",
        "# cbar.ax.tick_params(labelsize=15)  # Set fontsize for cbar ticks to 15\n",
        "\n",
        "# # Add a legend for Stage(TNM)\n",
        "# legend_labels = [Patch(facecolor=\"#B22222\", edgecolor=\"black\", label='PDA'),\n",
        "#                  Patch(facecolor=\"#1874CD\", edgecolor=\"black\", label='HC')]\n",
        "# legend = fig.legend(handles=legend_labels, loc=(0.92, 0.80), fontsize=15, frameon=False, title='Group')\n",
        "# legend.get_title().set_fontsize('15') # Legend title fontsize\n",
        "# legend.get_title().set_fontweight('bold') # Legend title font weight\n",
        "\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQA88gFS7sfI"
      },
      "outputs": [],
      "source": [
        "# Create a figure to contain the subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 15), gridspec_kw={'width_ratios': [1, 1], 'wspace': 0.007})\n",
        "\n",
        "# Verify if 'Stage(TNM)' exists in the DataFrame, if so, drop it\n",
        "if 'Stage(TNM)' in df_ts0.columns:\n",
        "    df_ts0 = df_ts0.drop(columns=['Stage(TNM)'])\n",
        "if 'Stage(TNM)' in df_ts1.columns:\n",
        "    df_ts1 = df_ts1.drop(columns=['Stage(TNM)'])\n",
        "\n",
        "# Create the first heatmap\n",
        "cax1 = sns.heatmap(df_ts0.T, ax=ax1, cmap=\"coolwarm\", cbar=False, vmin = -2, vmax=2)\n",
        "cax1.add_patch(plt.Rectangle((0, 0), df_ts0.shape[0], len(df_ts0.columns), fill=False, edgecolor=\"black\", lw=3))\n",
        "\n",
        "# Create the second heatmap\n",
        "cax2 = sns.heatmap(df_ts1.T, ax=ax2, cmap=\"coolwarm\", cbar=False, yticklabels=False, vmin = -2, vmax=2)\n",
        "cax2.add_patch(plt.Rectangle((0, 0), df_ts1.shape[0], len(df_ts1.columns), fill=False, edgecolor=\"black\", lw=3))\n",
        "\n",
        "# Add column names to the left of the first heatmap\n",
        "ax1.set_yticklabels(df_ts0.columns, rotation=0)\n",
        "\n",
        "# Remove x-axis labels and ticks from the first heatmap\n",
        "ax1.set_xticks([])\n",
        "ax1.set_xticklabels([])\n",
        "ax1.set_xlabel(\"\")\n",
        "\n",
        "# 기존 y축 라벨을 가져옵니다.\n",
        "old_y_labels = ax1.get_yticklabels()\n",
        "# 줄바꿈을 공백으로 대체합니다.\n",
        "new_y_labels = [label.get_text().replace(\"\\n\", \" \") for label in old_y_labels]\n",
        "# 새로운 y축 라벨을 설정합니다.\n",
        "ax1.set_yticklabels(new_y_labels, fontsize=13)\n",
        "\n",
        "# Hide the axes of the second heatmap\n",
        "ax2.axis('off')\n",
        "\n",
        "# Add a color bar above the heatmaps to separate samples based on Stage(TNM)\n",
        "colorbar_ax1 = fig.add_axes([ax1.get_position().x0, ax1.get_position().y1 + 0.005, ax1.get_position().width, 0.025])\n",
        "colorbar_ax2 = fig.add_axes([ax2.get_position().x0, ax2.get_position().y1 + 0.005, ax2.get_position().width, 0.025])\n",
        "\n",
        "colorbar_ax1.text(-0.085, 0.5, 'Group', verticalalignment='center', horizontalalignment='left', fontsize=14, va='center')\n",
        "colorbar_ax1.add_patch(plt.Rectangle((0, 0), 1, 1, facecolor=\"#1874CD\", edgecolor=\"black\", lw=3))\n",
        "colorbar_ax1.axis('off')\n",
        "\n",
        "colorbar_ax2.add_patch(plt.Rectangle((0, 0), 1, 1, facecolor=\"#B22222\", edgecolor=\"black\", lw=3))\n",
        "colorbar_ax2.axis('off')\n",
        "\n",
        "# Add a separate color bar (cbar) for the z-score\n",
        "cbar_ax = fig.add_axes([0.92, 0.6, 0.015, 0.1])\n",
        "cbar = plt.colorbar(ax2.collections[0], cax=cbar_ax)\n",
        "cbar_ax.set_title('Z-score', pad=10, fontsize=15, fontweight='bold')\n",
        "cbar.ax.tick_params(labelsize=15)  # Set fontsize for cbar ticks to 15\n",
        "\n",
        "# Add a legend for Stage(TNM)\n",
        "legend_labels = [Patch(facecolor=\"#1874CD\", edgecolor=\"black\", label='HC'),\n",
        "                Patch(facecolor=\"#B22222\", edgecolor=\"black\", label='PDA')]\n",
        "legend = fig.legend(handles=legend_labels, loc=(0.92, 0.80), fontsize=15, frameon=False, title='Group')\n",
        "legend.get_title().set_fontsize('15') # Legend title fontsize\n",
        "legend.get_title().set_fontweight('bold') # Legend title font weight\n",
        "\n",
        "plt.savefig(base_path + 'img/heatmap.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTMKk2eAE8Ay"
      },
      "outputs": [],
      "source": [
        "from matplotlib import gridspec\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "# # Create a figure to contain the subplots\n",
        "# fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 15))\n",
        "\n",
        "# Verify if 'Stage(TNM)' exists in the DataFrame, if so, drop it\n",
        "if 'Stage(TNM)' in df_ts00.columns:\n",
        "    df_ts00 = df_ts00.drop(columns=['Stage(TNM)'])\n",
        "if 'Stage(TNM)' in df_ts12.columns:\n",
        "    df_ts12 = df_ts12.drop(columns=['Stage(TNM)'])\n",
        "if 'Stage(TNM)' in df_ts34.columns:\n",
        "    df_ts34 = df_ts34.drop(columns=['Stage(TNM)'])\n",
        "\n",
        "# GridSpec 설정\n",
        "fig = plt.figure(figsize=(20, 15))\n",
        "gs = gridspec.GridSpec(1, 3, width_ratios=[len(df_ts00), len(df_ts12), len(df_ts34)])\n",
        "plt.subplots_adjust(wspace=0.007)\n",
        "\n",
        "# 각 heatmap을 그립니다.\n",
        "ax1 = plt.subplot(gs[0])\n",
        "ax2 = plt.subplot(gs[1])\n",
        "ax3 = plt.subplot(gs[2])\n",
        "\n",
        "# Create the first heatmap\n",
        "cax1 = sns.heatmap(df_ts00.T, ax=ax1, cmap=\"coolwarm\", cbar=False, vmin = -2, vmax=2)\n",
        "cax1.add_patch(plt.Rectangle((0, 0), df_ts00.shape[0], len(df_ts00.columns), fill=False, edgecolor=\"black\", lw=3))\n",
        "\n",
        "# Create the second heatmap\n",
        "cax2 = sns.heatmap(df_ts12.T, ax=ax2, cmap=\"coolwarm\", cbar=False, yticklabels=False, vmin = -2, vmax=2)\n",
        "cax2.add_patch(plt.Rectangle((0, 0), df_ts12.shape[0], len(df_ts12.columns), fill=False, edgecolor=\"black\", lw=3))\n",
        "\n",
        "# Create the second heatmap\n",
        "cax3 = sns.heatmap(df_ts34.T, ax=ax3, cmap=\"coolwarm\", cbar=False, yticklabels=False, vmin = -2, vmax=2)\n",
        "cax3.add_patch(plt.Rectangle((0, 0), df_ts34.shape[0], len(df_ts34.columns), fill=False, edgecolor=\"black\", lw=3))\n",
        "\n",
        "\n",
        "# Add column names to the left of the first heatmap\n",
        "ax1.set_yticklabels(df_ts00.columns, rotation=0)\n",
        "\n",
        "# Remove x-axis labels and ticks from the first heatmap\n",
        "ax1.set_xticks([])\n",
        "ax1.set_xticklabels([])\n",
        "ax1.set_xlabel(\"\")\n",
        "\n",
        "# Hide the axes of the second heatmap\n",
        "ax2.axis('off')\n",
        "ax3.axis('off')\n",
        "\n",
        "# Add a color bar above the heatmaps to separate samples based on Stage(TNM)\n",
        "colorbar_ax1 = fig.add_axes([ax1.get_position().x0, ax1.get_position().y1 + 0.003, ax1.get_position().width, 0.025])\n",
        "colorbar_ax2 = fig.add_axes([ax2.get_position().x0, ax2.get_position().y1 + 0.003, ax2.get_position().width, 0.025])\n",
        "colorbar_ax3 = fig.add_axes([ax3.get_position().x0, ax3.get_position().y1 + 0.003, ax3.get_position().width, 0.025])\n",
        "\n",
        "colorbar_ax1.text(-0.085, 0.5, 'Group', verticalalignment='center', horizontalalignment='left', fontsize=14, va='center')\n",
        "colorbar_ax1.add_patch(plt.Rectangle((0, 0), 1, 1, facecolor=\"#1874CD\", edgecolor=\"black\", lw=3))\n",
        "colorbar_ax1.axis('off')\n",
        "\n",
        "colorbar_ax2.add_patch(plt.Rectangle((0, 0), 1, 1, facecolor=\"#FF9614\", edgecolor=\"black\", lw=3))\n",
        "colorbar_ax2.axis('off')\n",
        "\n",
        "colorbar_ax3.add_patch(plt.Rectangle((0, 0), 1, 1, facecolor=\"#B22222\", edgecolor=\"black\", lw=3))\n",
        "colorbar_ax3.axis('off')\n",
        "\n",
        "# Add a separate color bar (cbar) for the z-score\n",
        "cbar_ax = fig.add_axes([0.92, 0.6, 0.015, 0.1])\n",
        "cbar = plt.colorbar(ax2.collections[0], cax=cbar_ax)\n",
        "cbar_ax.set_title('Z-score', pad=10, fontsize=15, fontweight='bold')\n",
        "cbar.ax.tick_params(labelsize=15)  # Set fontsize for cbar ticks to 15\n",
        "\n",
        "# Add a legend for Stage(TNM)\n",
        "legend_labels = [Patch(facecolor=\"#1874CD\", edgecolor=\"black\", label='HC'),\n",
        "                Patch(facecolor=\"#FF9614\", edgecolor=\"black\", label='(stage I/II)\\nPDA'),\n",
        "                Patch(facecolor=\"#B22222\", edgecolor=\"black\", label='(stage III/IV)\\nPDA')]\n",
        "legend = fig.legend(handles=legend_labels, loc=(0.895, 0.80), fontsize=15, frameon=False, title='Group')\n",
        "legend.get_title().set_fontsize('15') # Legend title fontsize\n",
        "legend.get_title().set_fontweight('bold') # Legend title font weight\n",
        "\n",
        "plt.savefig(base_path + 'img/heatmap2.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1uFJjWr95Oz"
      },
      "source": [
        "# 2. boxplot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_ts01.columns"
      ],
      "metadata": {
        "id": "ZCKfNfHif-D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sNuwtJu97HM"
      },
      "outputs": [],
      "source": [
        "# 데이터프레임을 'Stage(TNM)' 열을 x 축으로 하고 다른 열을 y 축으로 하는 형태로 재구성\n",
        "df_ts01_box1 = df_ts01.drop(columns=['TNFa',\n",
        "                                    'sEGFR',\n",
        "                                    'FGF-1',\n",
        "                                    'Ferritin',\n",
        "                                    'Kallikrein6',\n",
        "                                    'Endoglin',\n",
        "                                    'MIF',\n",
        "                                    'ALDH1A1',\n",
        "                                    'CD44',\n",
        "                                    'VEGF',\n",
        "                                    'sAXL',\n",
        "                                    'G-CSF',\n",
        "                                    'DKK-1',\n",
        "                                    'sFas',\n",
        "                                    'sPECAM-1',\n",
        "                                    'bHCG',\n",
        "                                    'IGFBP3',\n",
        "                                    'FGF2',\n",
        "                                    'sVEGFR1',\n",
        "                                    'sHer2',\n",
        "                                    'sE-Selectin',\n",
        "                                    'EpCAM',\n",
        "                                    'HE4',\n",
        "                                    ])\n",
        "\n",
        "df_ts01_box2 = df_ts01[['TNFa',\n",
        "                        'sEGFR',\n",
        "                        'FGF-1',\n",
        "                        'Ferritin',\n",
        "                        'Kallikrein6',\n",
        "                        'Endoglin',\n",
        "                        'MIF',\n",
        "                        'ALDH1A1',\n",
        "                        'CD44',\n",
        "                        'VEGF',\n",
        "                        'sAXL',\n",
        "                        'G-CSF',\n",
        "                        'DKK-1',\n",
        "                        'sFas',\n",
        "                        'sPECAM-1',\n",
        "                        'bHCG',\n",
        "                        'IGFBP3',\n",
        "                        'FGF2',\n",
        "                        'sVEGFR1',\n",
        "                        'sHer2',\n",
        "                        'sE-Selectin',\n",
        "                        'EpCAM',\n",
        "                        'HE4',\n",
        "                        'Stage(TNM)']]\n",
        "\n",
        "\n",
        "df_melted1 = df_ts01_box1.melt(id_vars='Stage(TNM)', var_name='Variable', value_name='Value')\n",
        "df_melted2 = df_ts01_box2.melt(id_vars='Stage(TNM)', var_name='Variable', value_name='Value')\n",
        "\n",
        "# boxplot 그리기\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.boxplot(x='Variable', y='Value', hue='Stage(TNM)', data=df_melted1, notch=0.5)\n",
        "plt.xticks(rotation=45)  # x 축 라벨 회전\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.boxplot(x='Variable', y='Value', hue='Stage(TNM)', data=df_melted2, notch=0.5)\n",
        "plt.xticks(rotation=45)  # x 축 라벨 회전\n",
        "\n",
        "# plt.ylim(-3,7.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_ts01"
      ],
      "metadata": {
        "id": "rVOIX3VjgWZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nxb0wLp4QAWq"
      },
      "outputs": [],
      "source": [
        "df_melted1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxMeuIooIsS0"
      },
      "outputs": [],
      "source": [
        "# boxplot 그리기\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.boxplot(x='Variable', y='Value', hue='Stage(TNM)', data=df_melted1, showcaps=False, flierprops={\"marker\": \".\", \"markersize\": 3})\n",
        "plt.xticks(rotation=45)  # x 축 라벨 회전\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.boxplot(x='Variable', y='Value', hue='Stage(TNM)', data=df_melted1, showcaps=False, flierprops={\"marker\": \".\", \"markersize\": 3})\n",
        "plt.xticks(rotation=45)  # x 축 라벨 회전\n",
        "\n",
        "plt.ylim(-3,7.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPEEA-hYKt7y"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(30,6))\n",
        "\n",
        "# 기본 boxplot 그리기\n",
        "sns.boxplot(x='Variable', y='Value', hue='Stage(TNM)', data=df_melted1, flierprops={\"marker\": \".\", \"markersize\": 8})\n",
        "\n",
        "# ylim을 벗어나는 이상치에 별표 표시\n",
        "ylim_lower, ylim_upper = -3, 7.5\n",
        "for i, variable in enumerate(df_melted1['Variable'].unique()):\n",
        "    for stage in df_melted1['Stage(TNM)'].unique():\n",
        "        sub_df = df_melted1[(df_melted1['Variable'] == variable) & (df_melted1['Stage(TNM)'] == stage)]\n",
        "        outliers = sub_df['Value'][(sub_df['Value'] < ylim_lower) | (sub_df['Value'] > ylim_upper)]\n",
        "\n",
        "        x_position = i - 0.2 if stage == 0 else i + 0.2  # hue에 따라 x 위치 조절\n",
        "        plt.scatter([x_position]*len(outliers), outliers, marker='*')\n",
        "\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylim(ylim_lower, ylim_upper)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcHknWDLNcgu"
      },
      "outputs": [],
      "source": [
        "# 색깔 설정\n",
        "my_palette = {0: '#1874CD', 1: '#B22222'}\n",
        "\n",
        "plt.figure(figsize=(20,6))\n",
        "sns.boxplot(x='Variable', y='Value', hue='Stage(TNM)', data=df_melted1, showcaps=False, flierprops={\"marker\": \".\", \"markersize\": 3}, palette=my_palette)\n",
        "plt.xticks(rotation=45)  # x 축 라벨 회전\n",
        "\n",
        "plt.ylim(-3,7.5)\n",
        "plt.legend().remove()  # 범례 제거\n",
        "plt.savefig(base_path + 'img/boxplot1.png')\n",
        "plt.show()\n",
        "\n",
        "#----\n",
        "\n",
        "plt.figure(figsize=(20,6))\n",
        "sns.boxplot(x='Variable', y='Value', hue='Stage(TNM)', data=df_melted2, showcaps=False, flierprops={\"marker\": \".\", \"markersize\": 3}, palette=my_palette)\n",
        "plt.xticks(rotation=45)  # x 축 라벨 회전\n",
        "\n",
        "plt.ylim(-3,7.5)\n",
        "plt.legend().remove()  # 범례 제거\n",
        "plt.savefig(base_path + 'img/boxplot2.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKKWQIouQKlA"
      },
      "outputs": [],
      "source": [
        "df_ts012 = df_ts.copy()\n",
        "# df_ts012.loc[(df_ts012['Stage(TNM)'] == 1) | (df_ts012['Stage(TNM)'] == 2), 'Stage(TNM)'] = 1\n",
        "# df_ts012.loc[(df_ts012['Stage(TNM)'] == 3) | (df_ts012['Stage(TNM)'] == 4), 'Stage(TNM)'] = 2\n",
        "\n",
        "df_ts012.loc[df_ts012['Stage(TNM)'].isin([1, 2]), 'Stage(TNM)'] = 1\n",
        "df_ts012.loc[df_ts012['Stage(TNM)'].isin([3, 4]), 'Stage(TNM)'] = 2\n",
        "\n",
        "\n",
        "df_ts012_box1 = df_ts012.drop(columns=['TNFa',\n",
        "                                    'sEGFR',\n",
        "                                    'FGF-1',\n",
        "                                    'Ferritin',\n",
        "                                    'Kallikrein6',\n",
        "                                    'Endoglin',\n",
        "                                    'MIF',\n",
        "                                    'ALDH1A1',\n",
        "                                    'CD44',\n",
        "                                    'VEGF',\n",
        "                                    'sAXL',\n",
        "                                    'G-CSF',\n",
        "                                    'DKK-1',\n",
        "                                    'sFas',\n",
        "                                    'sPECAM-1',\n",
        "                                    'bHCG',\n",
        "                                    'IGFBP3',\n",
        "                                    'FGF2',\n",
        "                                    'sVEGFR1',\n",
        "                                    'sHer2',\n",
        "                                    'sE-Selectin',\n",
        "                                    'EpCAM',\n",
        "                                    'HE4',\n",
        "                                    ])\n",
        "\n",
        "df_ts012_box2 = df_ts012[['TNFa',\n",
        "                        'sEGFR',\n",
        "                        'FGF-1',\n",
        "                        'Ferritin',\n",
        "                        'Kallikrein6',\n",
        "                        'Endoglin',\n",
        "                        'MIF',\n",
        "                        'ALDH1A1',\n",
        "                        'CD44',\n",
        "                        'VEGF',\n",
        "                        'sAXL',\n",
        "                        'G-CSF',\n",
        "                        'DKK-1',\n",
        "                        'sFas',\n",
        "                        'sPECAM-1',\n",
        "                        'bHCG',\n",
        "                        'IGFBP3',\n",
        "                        'FGF2',\n",
        "                        'sVEGFR1',\n",
        "                        'sHer2',\n",
        "                        'sE-Selectin',\n",
        "                        'EpCAM',\n",
        "                        'HE4',\n",
        "                        'Stage(TNM)']]\n",
        "\n",
        "# 데이터프레임을 'Stage(TNM)' 열을 x 축으로 하고 다른 열을 y 축으로 하는 형태로 재구성\n",
        "df_melted1 = df_ts012_box1.melt(id_vars='Stage(TNM)', var_name='Variable', value_name='Value')\n",
        "df_melted2 = df_ts012_box2.melt(id_vars='Stage(TNM)', var_name='Variable', value_name='Value')\n",
        "\n",
        "# 색깔 설정\n",
        "my_palette = {0: '#1874CD', 1:'#FF9614', 2: '#B22222'}\n",
        "\n",
        "plt.figure(figsize=(20,6))\n",
        "sns.boxplot(x='Variable', y='Value', hue='Stage(TNM)', data=df_melted1, showcaps=False, flierprops={\"marker\": \".\", \"markersize\": 3}, palette=my_palette)\n",
        "plt.xticks(rotation=45)  # x 축 라벨 회전\n",
        "\n",
        "plt.ylim(-3,7.5)\n",
        "plt.legend().remove()  # 범례 제거\n",
        "plt.savefig(base_path + 'img/boxplot3.png')\n",
        "plt.show()\n",
        "\n",
        "# ----\n",
        "\n",
        "# 색깔 설정\n",
        "# my_palette = {0: '#B22222', 1: '#1874CD'}\n",
        "\n",
        "plt.figure(figsize=(20,6))\n",
        "sns.boxplot(x='Variable', y='Value', hue='Stage(TNM)', data=df_melted2, showcaps=False, flierprops={\"marker\": \".\", \"markersize\": 3}, palette=my_palette)\n",
        "plt.xticks(rotation=45)  # x 축 라벨 회전\n",
        "\n",
        "plt.ylim(-3,7.5)\n",
        "plt.legend().remove()  # 범례 제거\n",
        "plt.savefig(base_path + 'img/boxplot4.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIGbLIuHY9_v"
      },
      "source": [
        "# SHAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtsAFnVHiuYX"
      },
      "outputs": [],
      "source": [
        "!pip install shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGB32sh3ktZn"
      },
      "outputs": [],
      "source": [
        "import xgboost\n",
        "import shap\n",
        "\n",
        "# 데이터 준비 (X, y)\n",
        "# ...\n",
        "X = df_ts.drop(columns='Stage(TNM)')\n",
        "y = df_ts['Stage(TNM)']\n",
        "\n",
        "# 모델 학습\n",
        "model = xgboost.train({\"learning_rate\": 0.01}, xgboost.DMatrix(X, label=y), 100)\n",
        "\n",
        "# SHAP explainer 객체 생성\n",
        "explainer = shap.Explainer(model)\n",
        "\n",
        "# SHAP 값 계산\n",
        "shap_values = explainer(X)  # Use X here, not df_ts\n",
        "\n",
        "# SHAP summary plot 그리기\n",
        "shap.summary_plot(shap_values, X)\n",
        "\n",
        "plt.savefig(base_path + 'img/SHAP.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euezikgJo4Mz"
      },
      "source": [
        "# AUC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6TxJuIw1EjQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import xgboost\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXmBupeQeOrT"
      },
      "outputs": [],
      "source": [
        "def rf_optimizer(trial, X, y, K):\n",
        "    # define parameter to tune\n",
        "    n_estimators = trial.suggest_categorical('n_estimators', [50, 100, 200])\n",
        "    max_depth = trial.suggest_int('max_depth', 4, 10)\n",
        "    max_features = trial.suggest_categorical('max_features', [0.6, 0.7, 0.8])\n",
        "\n",
        "\n",
        "    # set model\n",
        "    model = RandomForestClassifier(n_estimators=n_estimators,\n",
        "                                   max_depth=max_depth,\n",
        "                                   max_features=max_features,\n",
        "                                   criterion='log_loss',\n",
        "                                   class_weight='balanced'\n",
        "                                  )\n",
        "\n",
        "    # K-Fold Cross validation\n",
        "    folds = StratifiedKFold(n_splits=K, shuffle=True)\n",
        "    losses = []\n",
        "\n",
        "    for train_idx, val_idx in folds.split(X, y):\n",
        "        X_train = X.iloc[train_idx, :]\n",
        "        y_train = y.iloc[train_idx]\n",
        "\n",
        "        X_val = X.iloc[val_idx, :]\n",
        "        y_val = y.iloc[val_idx]\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        preds = model.predict_proba(X_val)\n",
        "        loss = evaluation_metric(y_val, preds)\n",
        "        losses.append(loss)\n",
        "\n",
        "\n",
        "    # return mean score of CV\n",
        "    return np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmsP5a4ieQ6w"
      },
      "outputs": [],
      "source": [
        "def xgb_optimizer(trial, X, y, K):\n",
        "\n",
        "    n_estimators = trial.suggest_categorical('n_estimators', [500, 1000, 2000])\n",
        "    max_depth = trial.suggest_int('max_depth', 4, 10)\n",
        "    colsample_bytree = trial.suggest_categorical('colsample_bytree', [0.5, 0.6, 0.7, 0.8])\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-3, 1e-2)\n",
        "    reg_lambda = trial.suggest_categorical('reg_lambda', [0.1, 0.5, 1, 2])\n",
        "\n",
        "\n",
        "    model = XGBClassifier(n_estimators=n_estimators,\n",
        "                          max_depth=max_depth,\n",
        "                          colsample_bytree=colsample_bytree,\n",
        "                          learning_rate=learning_rate,\n",
        "                          reg_lambda=reg_lambda)\n",
        "#                          scale_pos_weight=4.71)  ## we set class imbalance by using sampling method.\n",
        "\n",
        "\n",
        "    folds = StratifiedKFold(n_splits=K, shuffle=True)\n",
        "    losses = []\n",
        "\n",
        "    for train_idx, val_idx in folds.split(X, y):\n",
        "        X_train = X.iloc[train_idx, :]\n",
        "        y_train = y.iloc[train_idx]\n",
        "\n",
        "        X_val = X.iloc[val_idx, :]\n",
        "        y_val = y.iloc[val_idx]\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        preds = model.predict_proba(X_val)\n",
        "        loss = evaluation_metric(y_val, preds)\n",
        "        losses.append(loss)\n",
        "\n",
        "\n",
        "    return np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQjOl3HzeR96"
      },
      "outputs": [],
      "source": [
        "# ### SVM\n",
        "\n",
        "# if is_tuning:\n",
        "#     best_loss = 9999.0\n",
        "#     best_C = 0\n",
        "#     kernel = 'linear'\n",
        "#     folds = StratifiedKFold(n_splits=K, random_state=42, shuffle=True)\n",
        "\n",
        "#     # for Linear SVM\n",
        "#     for C in tqdm([1, 2, 5, 10, 100]):\n",
        "#         losses = []\n",
        "#         l_svm = LinearSVC(C=C, probability=True) ## cuml version. (faster model)\n",
        "\n",
        "#         for train_idx, val_idx in folds.split(X, y):\n",
        "#             X_train = X.iloc[train_idx, :]\n",
        "#             y_train = y.iloc[train_idx]\n",
        "#             X_val = X.iloc[val_idx, :]\n",
        "#             y_val = y.iloc[val_idx]\n",
        "\n",
        "#             l_svm.fit(X_train, y_train)\n",
        "#             preds = l_svm.predict_proba(X_val).values\n",
        "#             loss = evaluation_metric(y_val, preds)\n",
        "#             losses.append(loss)\n",
        "\n",
        "#         avg_loss = np.mean(losses)\n",
        "#         if avg_loss < best_loss:\n",
        "#             best_loss = avg_loss\n",
        "#             best_C = C\n",
        "\n",
        "#     # for SVM with RBF kernel.\n",
        "#     for C in tqdm([1, 2, 5, 10, 100]):\n",
        "#         losses = []\n",
        "#         r_svm = SVC(C=C, probability=True) ## cuml version. (with rbf kernel)\n",
        "\n",
        "#         for train_idx, val_idx in folds.split(X, y):\n",
        "#             X_train = X.iloc[train_idx, :]\n",
        "#             y_train = y.iloc[train_idx]\n",
        "#             X_val = X.iloc[val_idx, :]\n",
        "#             y_val = y.iloc[val_idx]\n",
        "\n",
        "#             r_svm.fit(X_train, y_train)\n",
        "#             preds = r_svm.predict_proba(X_val).values\n",
        "#             loss = evaluation_metric(y_val, preds)\n",
        "#             losses.append(loss)\n",
        "\n",
        "#         avg_loss = np.mean(losses)\n",
        "#         if avg_loss < best_loss:\n",
        "#             best_loss = avg_loss\n",
        "#             best_C = C\n",
        "#             kernel = 'rbf'\n",
        "\n",
        "#     print(\"SVM(%s) log loss : %.4f\" % (kernel, best_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfSRLYDX0Ihv"
      },
      "outputs": [],
      "source": [
        "# Dummy BINN Classifier (random predictions)\n",
        "class BINN:\n",
        "    def predict_proba(self, X):\n",
        "        return np.random.rand(len(X), 2)\n",
        "\n",
        "# # Load and preprocess your data (you'll replace this part with your actual data loading)\n",
        "# # Here df1 and df2 are your two dataframes\n",
        "# df1 = pd.read_csv('your_first_file.csv')\n",
        "# df2 = pd.read_csv('your_second_file.csv')\n",
        "\n",
        "# Initialize KNN imputer and Standard Scaler\n",
        "# imputer = KNNImputer(n_neighbors=5)\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# # Stage(TNM) 칼럼 빼기\n",
        "# df_tc_Stage = df_tc.pop('Stage(TNM)')\n",
        "\n",
        "# # Impute missing values\n",
        "# df_tc_imputed = pd.DataFrame(imputer.fit_transform(df_tc), columns=df_tc.columns)\n",
        "# df_td_imputed = pd.DataFrame(imputer.fit_transform(df_td), columns=df_td.columns)\n",
        "\n",
        "# # Remove outliers using IQR\n",
        "# df_tc_filtered = remove_outliers(df_tc_imputed)\n",
        "# df_td_filtered = remove_outliers(df_td_imputed)\n",
        "\n",
        "# # Standardize the data\n",
        "# df_tc_standardized = pd.DataFrame(scaler.fit_transform(df_tc_filtered), columns=df_tc_filtered.columns)\n",
        "# df_td_standardized = pd.DataFrame(scaler.fit_transform(df_td_filtered), columns=df_td_filtered.columns)\n",
        "\n",
        "# # Standardize the data\n",
        "# df_tc_standardized = pd.DataFrame(scaler.fit_transform(df_tc), columns=df_tc.columns)\n",
        "# df_td_standardized = pd.DataFrame(scaler.fit_transform(df_td), columns=df_td.columns)\n",
        "\n",
        "# df_tc_standardized['Stage(TNM)'] = df_tc_Stage\n",
        "\n",
        "# # Combine the data and prepare for modeling\n",
        "# df_combined = pd.concat([df_tc_standardized, df_td_standardized], ignore_index=True)\n",
        "\n",
        "# print(df_combined.columns)\n",
        "\n",
        "# df_combined['Stage(TNM)'] = df_combined['Stage(TNM)'].fillna(0)\n",
        "\n",
        "# df_combined01 = df_combined.copy()\n",
        "# df_combined01.loc[df_combined01['Stage(TNM)'] >= 1, 'Stage(TNM)'] = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imsl8sn-1IJY"
      },
      "source": [
        "### BINN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQ9f-7G61iw_"
      },
      "outputs": [],
      "source": [
        "pip install binn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uEwAxgk1JzW"
      },
      "outputs": [],
      "source": [
        "# from binn import BINN, Network\n",
        "# from binn import BINNClassifier\n",
        "\n",
        "# network = Network(\n",
        "#     input_data=df_t,\n",
        "#     pathways=None,\n",
        "#     mapping=None\n",
        "#     # verbose=True\n",
        "# )\n",
        "\n",
        "# binn = BINNClassifier(\n",
        "#     pathways=network,\n",
        "#     n_layers=4,\n",
        "#     dropout=0.2,\n",
        "#     validate=True,\n",
        "#     epochs=10,\n",
        "#     threads=10,\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qzeePBOwaic"
      },
      "source": [
        "### X, y 분리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fbPY_TXwdl8"
      },
      "outputs": [],
      "source": [
        "# Split the data\n",
        "X01 = df_t01.drop('Stage(TNM)', axis=1)\n",
        "y01 = df_t01['Stage(TNM)']\n",
        "\n",
        "X_train01, X_test01, y_train01, y_test01 = train_test_split(X01, y01, test_size=0.2, random_state=0) # 랜덤 스테이트 42?\n",
        "\n",
        "if is_dropcol:\n",
        "  X01 = df_t01_drop.drop('Stage(TNM)', axis=1)\n",
        "  y01 = df_t01_drop['Stage(TNM)']\n",
        "\n",
        "  X_train01, X_test01, y_train01, y_test01 = train_test_split(X01, y01, test_size=0.2, random_state=0) # 랜덤 스테이트 42?\n",
        "\n",
        "# if is_012:\n",
        "#   X = df_t012.drop('Stage(TNM)', axis=1)\n",
        "#   y = df_t012['Stage(TNM)']\n",
        "\n",
        "#   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) # 랜덤 스테이트 42?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y01"
      ],
      "metadata": {
        "id": "3VgTLxilDGOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 학습"
      ],
      "metadata": {
        "id": "RPKdErrt_l84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from binn import Network, BINN\n",
        "# from binn import BINNClassifier\n",
        "# import pandas as pd\n",
        "\n",
        "# # Create a Network object with only input_data\n",
        "# # Assuming that pathways and mapping are optional parameters, we leave them out\n",
        "\n",
        "# pathway = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/크몽/바이오/pathways.csv')\n",
        "# translation = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/크몽/바이오/translation.csv')\n",
        "\n",
        "# network = Network(\n",
        "#     input_data=df_t,\n",
        "#     pathways=pathway,  # Empty DataFrame as there's no pathway info\n",
        "#     mapping=translation,  # Empty DataFrame as there's no mapping info\n",
        "#     input_data_column = \"Stage(TNM)\", # specify the column for entities in input data\n",
        "#     source_column = \"child\", # defined by our pathways-file\n",
        "#     target_column = \"parent\",\n",
        "# )"
      ],
      "metadata": {
        "id": "vmxG6XMTFgBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from binn import BINN, Network, BINNClassifier\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# network = Network(\n",
        "#     input_data=df_t,  # X_train from the previous split\n",
        "#     pathways=pd.DataFrame(),  # Empty DataFrame as there's no pathway info\n",
        "#     mapping=pd.DataFrame(),  # Empty DataFrame as there's no mapping info\n",
        "#     input_data_column= 'CA19-9',\n",
        "#     source_column = 'Stage(TNM)',\n",
        "#     # target_column = \"Stage(TNM)\"\n",
        "# )"
      ],
      "metadata": {
        "id": "kIj_OMyz-oNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# binn = BINNClassifier(\n",
        "#     pathways=network,\n",
        "#     n_layers=4,\n",
        "#     dropout=0.2,\n",
        "#     validate=True,\n",
        "#     epochs=10,\n",
        "#     threads=10,\n",
        "# )\n",
        "# binn.fit(X_train, y_train)\n",
        "\n",
        "# y_pred_binn = binn.predict(X_test)\n",
        "\n",
        "# conf_matrix_binn = confusion_matrix(y_test, y_pred_binn)\n",
        "# class_report_binn = classification_report(y_test, y_pred_binn)\n",
        "\n",
        "# conf_matrix_binn, class_report_binn"
      ],
      "metadata": {
        "id": "JeBVGD-9CP48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9XhNW0hf1df"
      },
      "outputs": [],
      "source": [
        "# rf_run = RandomForestClassifier(random_state=0, max_depth=5, min_simples_leaf=8,min_samples_split=8,n_estimators=200)\n",
        "# rf_run.fit\n",
        "\n",
        "# folds = StratifiedKFold(n_splits=K, shuffle=True)\n",
        "# losses = []"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ROC 커브"
      ],
      "metadata": {
        "id": "thKdWfpvJYQM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VodDRDBdfuk3"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Initialize classifiers with optimized hyperparameters\n",
        "classifiers_optimized = {\n",
        "    'XGBoost': xgboost.XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_estimators=50, max_depth=3),\n",
        "    'SVM': SVC(probability=True, C=0.5),\n",
        "    'K-NN': KNeighborsClassifier(n_neighbors=3),\n",
        "    'LightGBM': lgb.LGBMClassifier(n_estimators=50, max_depth=3,  verbose=-1),\n",
        "    'Logistic Regression': LogisticRegression(),  # Add Logistic Regression\n",
        "    'Random Forest': RandomForestClassifier(random_state=0, max_depth=5, min_samples_leaf=8, min_samples_split=8, n_estimators=200),\n",
        "}\n",
        "\n",
        "colors = {\n",
        "    'XGBoost': '#3232FF',  # Blue\n",
        "    'SVM': '#5050FF',  # Green\n",
        "    'K-NN': '#1E82FF',  # Red\n",
        "    'LightGBM': '#E6A055',  # Purple\n",
        "    'Logistic Regression': '#EF904C',  # Orange\n",
        "    'Random Forest': '#B93232'  # Cyan\n",
        "}\n",
        "\n",
        "# Initialize figure for ROC curve\n",
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "# Initialize dictionaries to store FPR, TPR, and AUC values for each classifier\n",
        "roc_curves = defaultdict(list)\n",
        "roc_aucs = defaultdict(list)\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "# Loop through classifiers to plot ROC curve\n",
        "for name, clf in classifiers_optimized.items():\n",
        "    # print(y_train)\n",
        "    clf.fit(X_train01, y_train01)\n",
        "    y_pred_proba = clf.predict_proba(X_test01)[:, 1]\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_test01, y_pred_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Interpolate the TPR to be the same length as mean_fpr\n",
        "    interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
        "    interp_tpr[0] = 0.0\n",
        "\n",
        "    roc_curves[name].append(interp_tpr)\n",
        "    roc_aucs[name].append(roc_auc)\n",
        "\n",
        "    # plt.plot(fpr, tpr, label=f\"{name} (AUC = {roc_auc:.2f})\")\n",
        "\n",
        "for name, tprs in roc_curves.items():\n",
        "    color = colors.get(name, 'aqua')  # Default color if not found\n",
        "    mean_tpr = np.mean(tprs, axis=0)\n",
        "    mean_tpr[-1] = 1.0\n",
        "    mean_auc = auc(mean_fpr, mean_tpr)\n",
        "    std_auc = np.std(roc_aucs[name])\n",
        "    std_tpr = np.std(tprs, axis=0)\n",
        "\n",
        "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "\n",
        "    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color=color, alpha=.2)\n",
        "    plt.plot(mean_fpr, mean_tpr, color=color, label=f\"{name}: {mean_auc:.2f} +/- {std_auc:.2f}\")\n",
        "\n",
        "# Plot random chance line\n",
        "plt.plot([0, 1], [0, 1], 'k--', label=\"Chance (AUC = 0.5)\")\n",
        "\n",
        "# Set plot options\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('1 - Specificity (False Positive Rate)')\n",
        "plt.ylabel('Sensitivity (True Positive Rate)')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Initialize classifiers with optimized hyperparameters\n",
        "classifiers_optimized = {\n",
        "    'XGBoost': xgboost.XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_estimators=50, max_depth=3),\n",
        "    'SVM': SVC(probability=True, C=0.5),\n",
        "    'K-NN': KNeighborsClassifier(n_neighbors=3),\n",
        "    'LightGBM': lgb.LGBMClassifier(n_estimators=50, max_depth=3,  verbose=-1),\n",
        "    'Logistic Regression': LogisticRegression(),  # Add Logistic Regression\n",
        "    'Random Forest': RandomForestClassifier(random_state=0, max_depth=5, min_samples_leaf=8, min_samples_split=8, n_estimators=200),\n",
        "}\n",
        "\n",
        "colors = {\n",
        "    'XGBoost': '#3232FF',  # Blue\n",
        "    'SVM': '#5050FF',  # Green\n",
        "    'K-NN': '#1E82FF',  # Red\n",
        "    'LightGBM': '#E6A055',  # Purple\n",
        "    'Logistic Regression': '#EF904C',  # Orange\n",
        "    'Random Forest': '#B93232'  # Cyan\n",
        "}\n",
        "\n",
        "# 초기 통합 Confusion Matrix 설정\n",
        "total_cm = np.zeros((2, 2))  # 이진 분류의 경우 2x2 행렬\n",
        "\n",
        "# Confusion Matrix를 저장할 딕셔너리\n",
        "cm_dict = {}\n",
        "\n",
        "# Initialize figure for ROC curve\n",
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "# Initialize dictionaries to store FPR, TPR, and AUC values for each classifier\n",
        "roc_curves = defaultdict(list)\n",
        "roc_aucs = defaultdict(list)\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "# Loop through classifiers to plot ROC curve\n",
        "for name, clf in classifiers_optimized.items():\n",
        "    # print(y_train)\n",
        "    clf.fit(X_train01, y_train01)\n",
        "    y_pred_proba = clf.predict_proba(X_test01)[:, 1]\n",
        "    y_pred = clf.predict(X_test01)\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_test01, y_pred_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Confusion Matrix 계산\n",
        "    cm = confusion_matrix(y_test01, y_pred)  # 실제 레이블과 예측 레이블을 사용\n",
        "\n",
        "    # 통합 Confusion Matrix에 더하기\n",
        "    total_cm += cm\n",
        "\n",
        "    # Interpolate the TPR to be the same length as mean_fpr\n",
        "    interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
        "    interp_tpr[0] = 0.0\n",
        "\n",
        "    roc_curves[name].append(interp_tpr)\n",
        "    roc_aucs[name].append(roc_auc)\n",
        "\n",
        "    # plt.plot(fpr, tpr, label=f\"{name} (AUC = {roc_auc:.2f})\")\n",
        "\n",
        "for name, tprs in roc_curves.items():\n",
        "    color = colors.get(name, 'aqua')  # Default color if not found\n",
        "    mean_tpr = np.mean(tprs, axis=0)\n",
        "    mean_tpr[-1] = 1.0\n",
        "    mean_auc = auc(mean_fpr, mean_tpr)\n",
        "    std_auc = np.std(roc_aucs[name])\n",
        "    std_tpr = np.std(tprs, axis=0)\n",
        "\n",
        "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "\n",
        "    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color=color, alpha=.2)\n",
        "    plt.plot(mean_fpr, mean_tpr, color=color, label=f\"{name}: {mean_auc:.2f} +/- {std_auc:.2f}\")\n",
        "\n",
        "# 변화율 계산 (예: True Positive Rate 변화율)\n",
        "tpr_change_rate = total_cm[1, 1] / (total_cm[1, 1] + total_cm[1, 0])\n",
        "fpr_change_rate = total_cm[0, 1] / (total_cm[0, 1] + total_cm[0, 0])\n",
        "\n",
        "# 통합 Confusion Matrix를 그립니다.\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(total_cm, annot=True, fmt='.2f', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title(f'Total Confusion Matrix: TPR Change Rate = {tpr_change_rate:.2f}, FPR Change Rate = {fpr_change_rate:.2f}')\n",
        "plt.show()\n",
        "\n",
        "# Plot random chance line\n",
        "plt.plot([0, 1], [0, 1], 'k--', label=\"Chance (AUC = 0.5)\")\n",
        "\n",
        "# Set plot options\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('1 - Specificity (False Positive Rate)')\n",
        "plt.ylabel('Sensitivity (True Positive Rate)')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(classification_report(y_test01, y_pred))\n"
      ],
      "metadata": {
        "id": "V3TYu10uXj3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 타겟 01, 컬럼 3개 사용했을 때"
      ],
      "metadata": {
        "id": "8Qgo1y71Ja7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "colors = {\n",
        "    'XGBoost': '#3232FF',\n",
        "    'SVM': '#5050FF',\n",
        "    'K-NN': '#1E82FF',\n",
        "    'LightGBM': '#E6A055',\n",
        "    'Logistic Regression': '#EF904C',\n",
        "    'Random Forest': '#B93232',\n",
        "}\n",
        "\n",
        "# Initialize figure for ROC curve\n",
        "plt.figure(figsize=(6, 6))\n",
        "\n",
        "# Number of splits\n",
        "n_splits = 3\n",
        "cv = StratifiedKFold(n_splits=n_splits)\n",
        "\n",
        "# Initialize dictionaries to store FPR, TPR, and AUC values for each classifier\n",
        "roc_curves = defaultdict(list)\n",
        "roc_aucs = defaultdict(list)\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "# Loop through each fold\n",
        "for train_index, test_index in cv.split(X_train01, y_train01):\n",
        "    X_train01_fold, X_test_fold = X_train01.iloc[train_index], X_train01.iloc[test_index]\n",
        "    y_train01_fold, y_test_fold = y_train01.iloc[train_index], y_train01.iloc[test_index]\n",
        "\n",
        "    # Loop through classifiers\n",
        "    for name, clf in classifiers_optimized.items():\n",
        "        clf.fit(X_train01_fold, y_train01_fold)\n",
        "        y_pred_proba = clf.predict_proba(X_test_fold)[:, 1]\n",
        "\n",
        "        fpr, tpr, _ = roc_curve(y_test_fold, y_pred_proba)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        # Interpolate the TPR to be the same length as mean_fpr\n",
        "        interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
        "        interp_tpr[0] = 0.0\n",
        "\n",
        "        roc_curves[name].append(interp_tpr)\n",
        "        roc_aucs[name].append(roc_auc)\n",
        "\n",
        "for name, tprs in roc_curves.items():\n",
        "    color = colors.get(name, 'aqua')  # Default color if not found\n",
        "    mean_tpr = np.mean(tprs, axis=0)\n",
        "    mean_tpr[-1] = 1.0\n",
        "    mean_auc = auc(mean_fpr, mean_tpr)\n",
        "    std_auc = np.std(roc_aucs[name])\n",
        "    std_tpr = np.std(tprs, axis=0)\n",
        "\n",
        "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "\n",
        "    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color=color, alpha=.2)\n",
        "    plt.plot(mean_fpr, mean_tpr, color=color, label=f\"{name}: {mean_auc:.2f} +/- {std_auc:.2f}\")\n",
        "\n",
        "\n",
        "# Plot random chance line\n",
        "# plt.plot([0, 1], [0, 1], 'k--', label=\"Chance (AUC = 0.5)\")\n",
        "\n",
        "# Set plot options\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.02])\n",
        "plt.xlabel('1 - Specificity (False Positive Rate)')\n",
        "plt.ylabel('Sensitivity (True Positive Rate)')\n",
        "# plt.title('Receiver Operating Characteristic with k-Fold Cross-Validation')\n",
        "\n",
        "# Hide the right and top spines\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "\n",
        "plt.legend(loc=\"lower right\", frameon=False, title='AUC', title_fontsize=11)\n",
        "plt.savefig(base_path + 'ROC1.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sAmYcOAUpF44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "colors = {\n",
        "    'XGBoost': '#3232FF',\n",
        "    'SVM': '#5050FF',\n",
        "    'K-NN': '#1E82FF',\n",
        "    'LightGBM': '#E6A055',\n",
        "    'Logistic Regression': '#EF904C',\n",
        "    'Random Forest': '#B93232',\n",
        "}\n",
        "\n",
        "# Initialize figure for ROC curve\n",
        "plt.figure(figsize=(6, 6))\n",
        "\n",
        "# 초기 통합 Confusion Matrix 설정\n",
        "total_cm = np.zeros((2, 2))  # 이진 분류의 경우 2x2 행렬\n",
        "\n",
        "# Confusion Matrix를 저장할 딕셔너리\n",
        "cm_dict = {}\n",
        "\n",
        "# Number of splits\n",
        "n_splits = 3\n",
        "cv = StratifiedKFold(n_splits=n_splits)\n",
        "\n",
        "# Initialize dictionaries to store FPR, TPR, and AUC values for each classifier\n",
        "roc_curves = defaultdict(list)\n",
        "roc_aucs = defaultdict(list)\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "# Loop through each fold\n",
        "for train_index, test_index in cv.split(X_train01, y_train01):\n",
        "    X_train01_fold, X_test_fold = X_train01.iloc[train_index], X_train01.iloc[test_index]\n",
        "    y_train01_fold, y_test_fold = y_train01.iloc[train_index], y_train01.iloc[test_index]\n",
        "\n",
        "    # Loop through classifiers\n",
        "    for name, clf in classifiers_optimized.items():\n",
        "        clf.fit(X_train01_fold, y_train01_fold)\n",
        "        y_pred_proba = clf.predict_proba(X_test_fold)[:, 1]\n",
        "\n",
        "        fpr, tpr, _ = roc_curve(y_test_fold, y_pred_proba)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        # Confusion Matrix 계산\n",
        "        cm = confusion_matrix(y_test01, y_pred)  # 실제 레이블과 예측 레이블을 사용\n",
        "\n",
        "        # 통합 Confusion Matrix에 더하기\n",
        "        total_cm += cm\n",
        "\n",
        "        # Interpolate the TPR to be the same length as mean_fpr\n",
        "        interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
        "        interp_tpr[0] = 0.0\n",
        "\n",
        "        roc_curves[name].append(interp_tpr)\n",
        "        roc_aucs[name].append(roc_auc)\n",
        "\n",
        "for name, tprs in roc_curves.items():\n",
        "    color = colors.get(name, 'aqua')  # Default color if not found\n",
        "    mean_tpr = np.mean(tprs, axis=0)\n",
        "    mean_tpr[-1] = 1.0\n",
        "    mean_auc = auc(mean_fpr, mean_tpr)\n",
        "    std_auc = np.std(roc_aucs[name])\n",
        "    std_tpr = np.std(tprs, axis=0)\n",
        "\n",
        "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "\n",
        "    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color=color, alpha=.2)\n",
        "    plt.plot(mean_fpr, mean_tpr, color=color, label=f\"{name}: {mean_auc:.2f} +/- {std_auc:.2f}\")\n",
        "\n",
        "\n",
        "# Plot random chance line\n",
        "# plt.plot([0, 1], [0, 1], 'k--', label=\"Chance (AUC = 0.5)\")\n",
        "\n",
        "# # 변화율 계산 (예: True Positive Rate 변화율)\n",
        "# tpr_change_rate = total_cm[1, 1] / (total_cm[1, 1] + total_cm[1, 0])\n",
        "# fpr_change_rate = total_cm[0, 1] / (total_cm[0, 1] + total_cm[0, 0])\n",
        "\n",
        "# 전체 샘플 수 계산 (각 열 별로)\n",
        "total_samples_per_class = np.sum(total_cm, axis=0)\n",
        "\n",
        "# 각 셀의 값을 해당 열의 총합으로 나누어 퍼센트로 변환\n",
        "total_cm_percentage_per_class = (total_cm / total_samples_per_class) * 100\n",
        "\n",
        "# Manually create the annotations using list comprehension\n",
        "rows, cols = total_cm_percentage_per_class.shape\n",
        "rounded_percentage_per_class = [[f\"{total_cm_percentage_per_class[i, j]:.0f}%\" for j in range(cols)] for i in range(rows)]\n",
        "\n",
        "# 통합 Confusion Matrix를 그립니다.\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(total_cm_percentage_per_class, annot=rounded_percentage_per_class, fmt='', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'], annot_kws={\"fontsize\": 14}, cbar=False)\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title(f'Total Confusion Matrix: TPR Change Rate = {tpr_change_rate:.2f}, FPR Change Rate = {fpr_change_rate:.2f}')\n",
        "plt.show()\n",
        "\n",
        "# Set plot options\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.02])\n",
        "plt.xlabel('1 - Specificity (False Positive Rate)')\n",
        "plt.ylabel('Sensitivity (True Positive Rate)')\n",
        "# plt.title('Receiver Operating Characteristic with k-Fold Cross-Validation')\n",
        "\n",
        "# Hide the right and top spines\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "\n",
        "plt.legend(loc=\"lower right\", frameon=False, title='AUC', title_fontsize=11)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vp8cOICWeYDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "colors = {\n",
        "    'XGBoost': '#3232FF',\n",
        "    'SVM': '#5050FF',\n",
        "    'K-NN': '#1E82FF',\n",
        "    'LightGBM': '#E6A055',\n",
        "    'Logistic Regression': '#EF904C',\n",
        "    'Random Forest': '#B93232',\n",
        "}\n",
        "\n",
        "# Initialize figure for ROC curve\n",
        "plt.figure(figsize=(6, 6))\n",
        "\n",
        "# 초기 통합 Confusion Matrix 설정\n",
        "total_cm = np.zeros((2, 2))  # 이진 분류의 경우 2x2 행렬\n",
        "\n",
        "# Confusion Matrix를 저장할 딕셔너리\n",
        "cm_dict = {}\n",
        "cms = []\n",
        "\n",
        "# Number of splits\n",
        "n_splits = 3\n",
        "cv = StratifiedKFold(n_splits=n_splits)\n",
        "\n",
        "# Initialize dictionaries to store FPR, TPR, and AUC values for each classifier\n",
        "roc_curves = defaultdict(list)\n",
        "roc_aucs = defaultdict(list)\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "# Loop through each fold\n",
        "for train_index, test_index in cv.split(X_train01, y_train01):\n",
        "    X_train01_fold, X_test_fold = X_train01.iloc[train_index], X_train01.iloc[test_index]\n",
        "    y_train01_fold, y_test_fold = y_train01.iloc[train_index], y_train01.iloc[test_index]\n",
        "\n",
        "    # Loop through classifiers\n",
        "    for name, clf in classifiers_optimized.items():\n",
        "        clf.fit(X_train01_fold, y_train01_fold)\n",
        "        y_pred_proba = clf.predict_proba(X_test_fold)[:, 1]\n",
        "        y_pred = clf.predict(X_test_fold)\n",
        "\n",
        "        fpr, tpr, _ = roc_curve(y_test_fold, y_pred_proba)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        # Confusion Matrix 계산\n",
        "        cm = confusion_matrix(y_test_fold, y_pred)  # 실제 레이블과 예측 레이블을 사용\n",
        "\n",
        "            # Confusion Matrix를 리스트에 저장\n",
        "        cms.append(cm)\n",
        "\n",
        "        # 통합 Confusion Matrix에 더하기\n",
        "        total_cm += cm\n",
        "\n",
        "        # Interpolate the TPR to be the same length as mean_fpr\n",
        "        interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
        "        interp_tpr[0] = 0.0\n",
        "\n",
        "        roc_curves[name].append(interp_tpr)\n",
        "        roc_aucs[name].append(roc_auc)\n",
        "\n",
        "for name, tprs in roc_curves.items():\n",
        "    color = colors.get(name, 'aqua')  # Default color if not found\n",
        "    mean_tpr = np.mean(tprs, axis=0)\n",
        "    mean_tpr[-1] = 1.0\n",
        "    mean_auc = auc(mean_fpr, mean_tpr)\n",
        "    std_auc = np.std(roc_aucs[name])\n",
        "    std_tpr = np.std(tprs, axis=0)\n",
        "\n",
        "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "\n",
        "    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color=color, alpha=.2)\n",
        "    plt.plot(mean_fpr, mean_tpr, color=color, label=f\"{name}: {mean_auc:.2f} +/- {std_auc:.2f}\")\n",
        "\n",
        "\n",
        "# Plot random chance line\n",
        "# plt.plot([0, 1], [0, 1], 'k--', label=\"Chance (AUC = 0.5)\")\n",
        "\n",
        "# # 변화율 계산 (예: True Positive Rate 변화율)\n",
        "# tpr_change_rate = total_cm[1, 1] / (total_cm[1, 1] + total_cm[1, 0])\n",
        "# fpr_change_rate = total_cm[0, 1] / (total_cm[0, 1] + total_cm[0, 0])\n",
        "\n",
        "# 각 fold에서의 Confusion Matrix를 numpy array로 변환\n",
        "cms = np.array(cms)\n",
        "\n",
        "\n",
        "# 열별로 평균과 표준편차 계산\n",
        "mean_cms = np.mean(cms, axis=0)\n",
        "std_cms = np.std(cms, axis=0)\n",
        "\n",
        "# 열별로 퍼센트와 변화율 계산\n",
        "total_samples_per_class = np.sum(mean_cms, axis=0)\n",
        "mean_percentage = (mean_cms / total_samples_per_class) * 100\n",
        "std_percentage = (std_cms / total_samples_per_class) * 10\n",
        "\n",
        "# Manually create the annotations using list comprehension\n",
        "# Manually get the shape of the mean_percentage list\n",
        "rows = len(mean_percentage)\n",
        "cols = len(mean_percentage[0]) if rows > 0 else 0\n",
        "# Manually create the annotations using list comprehension with floor rounding for std\n",
        "annotations = [[f\"{mean_percentage[i][j]:.0f}±{math.floor(std_percentage[i][j])}%\" for j in range(cols)] for i in range(rows)]\n",
        "\n",
        "\n",
        "# # Calculate percentages\n",
        "# mean_percentage = (mean_cms / np.sum(mean_cms, axis=1, keepdims=True)) * 100\n",
        "# std_percentage = (std_cms / np.sum(std_cms, axis=1, keepdims=True)) * 100\n",
        "\n",
        "# # Create annotations\n",
        "# rows, cols = mean_percentage.shape\n",
        "# annotations = [[f\"{mean_percentage[i, j]:.0f}±{std_percentage[i, j]:.0f}%\" for j in range(cols)] for i in range(rows)]\n",
        "\n",
        "\n",
        "# # 전체 샘플 수 계산 (각 열 별로)\n",
        "# total_samples_per_class = np.sum(total_cm, axis=0)\n",
        "\n",
        "# # 각 셀의 값을 해당 열의 총합으로 나누어 퍼센트로 변환\n",
        "# total_cm_percentage_per_class = (total_cm / total_samples_per_class) * 100\n",
        "\n",
        "# # Manually create the annotations using list comprehension\n",
        "# rows, cols = total_cm_percentage_per_class.shape\n",
        "# rounded_percentage_per_class = [[f\"{total_cm_percentage_per_class[i, j]:.0f}%\" for j in range(cols)] for i in range(rows)]\n",
        "\n",
        "\n",
        "# 통합 Confusion Matrix를 그립니다.\n",
        "plt.figure(figsize=(6, 6))\n",
        "# sns.heatmap(total_cm_percentage_per_class, annot=rounded_percentage_per_class, fmt='', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'], annot_kws={\"fontsize\": 14}, cbar=False)\n",
        "# sns.heatmap(mean_percentage, annot=annotations, fmt='', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'], annot_kws={\"fontsize\": 14}, cbar=False)\n",
        "# plt.ylabel('Actual')\n",
        "# plt.xlabel('Predicted')\n",
        "# plt.title(f'Total Confusion Matrix: TPR Change Rate = {tpr_change_rate:.2f}, FPR Change Rate = {fpr_change_rate:.2f}')\n",
        "# plt.show()\n",
        "\n",
        "sns.heatmap(mean_percentage.T, annot=np.array(annotations).T, fmt='', cmap='coolwarm', xticklabels=['More severe', 'Less severe'], yticklabels=['More severe', 'Less severe'], annot_kws={\"fontsize\": 14}, cbar=False)\n",
        "plt.xlabel('True')\n",
        "plt.ylabel('Predicted')\n",
        "# plt.title('Total Confusion Matrix with Change Rate')\n",
        "plt.show()\n",
        "\n",
        "# Set plot options\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.02])\n",
        "plt.xlabel('1 - Specificity (False Positive Rate)')\n",
        "plt.ylabel('Sensitivity (True Positive Rate)')\n",
        "# plt.title('Receiver Operating Characteristic with k-Fold Cross-Validation')\n",
        "\n",
        "# Hide the right and top spines\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "\n",
        "plt.legend(loc=\"lower right\", frameon=False, title='AUC', title_fontsize=11)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FjzALMXMhiPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fpr"
      ],
      "metadata": {
        "id": "0xJ2QUSr4iDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize figure for PR curve\n",
        "plt.figure(figsize=(6, 6))\n",
        "\n",
        "# Number of splits\n",
        "n_splits = 3\n",
        "cv = StratifiedKFold(n_splits=n_splits)\n",
        "\n",
        "# Initialize dictionaries to store FPR, TPR, and AUC values for each classifier\n",
        "pr_curves = defaultdict(list)\n",
        "pr_aucs = defaultdict(list)\n",
        "mean_recall = np.linspace(0, 1, 100)\n",
        "\n",
        "# Loop through each fold\n",
        "for train_index, test_index in cv.split(X_train01, y_train01):\n",
        "    X_train01_fold, X_test_fold = X_train01.iloc[train_index], X_train01.iloc[test_index]\n",
        "    y_train01_fold, y_test_fold = y_train01.iloc[train_index], y_train01.iloc[test_index]\n",
        "\n",
        "    # Loop through classifiers\n",
        "    for name, clf in classifiers_optimized.items():\n",
        "        clf.fit(X_train01_fold, y_train01_fold)\n",
        "        y_pred_proba = clf.predict_proba(X_test_fold)[:, 1]\n",
        "\n",
        "        precision, recall, _ = precision_recall_curve(y_test_fold, y_pred_proba)\n",
        "        avg_precision = average_precision_score(y_test_fold, y_pred_proba)\n",
        "\n",
        "        # Interpolate the precision to be the same length as mean_recall\n",
        "        interp_precision = np.interp(mean_recall, recall[::-1], precision[::-1])\n",
        "\n",
        "        pr_curves[name].append(interp_precision)\n",
        "        pr_aucs[name].append(avg_precision)\n",
        "\n",
        "# Plot average PR curves\n",
        "for name, precisions in pr_curves.items():\n",
        "    color = colors.get(name, 'aqua')  # Default color if not found\n",
        "    mean_precision = np.mean(precisions, axis=0)\n",
        "    std_precision = np.std(precisions, axis=0)\n",
        "    mean_auc = np.mean(pr_aucs[name])\n",
        "    std_auc = np.std(pr_aucs[name])\n",
        "\n",
        "    precision_upper = np.minimum(mean_precision + std_precision, 1)\n",
        "    precision_lower = np.maximum(mean_precision - std_precision, 0)\n",
        "\n",
        "    plt.fill_between(mean_recall, precision_lower, precision_upper, color=color, alpha=.2)\n",
        "    plt.plot(mean_recall, mean_precision, color=color,\n",
        "             label=f\"{name}: {mean_auc:.2f} +/- {std_auc:.2f}\")\n",
        "\n",
        "\n",
        "# Set plot options\n",
        "plt.xlim([0.0, 1.05])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "\n",
        "# Hide the right and top spines\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "\n",
        "plt.legend(loc=\"lower right\", frameon=False, title='AUC', title_fontsize=11)\n",
        "plt.savefig(base_path + 'ROC_Recall_1.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OmOnBO-jvrRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Initialize figure for Precision-Recall curve\n",
        "# plt.figure(figsize=(6, 6))\n",
        "\n",
        "# # Create a uniform grid for recall values\n",
        "# mean_recall = np.linspace(0, 1, 100)\n",
        "\n",
        "# # Initialize dictionaries to store interpolated precision\n",
        "# interp_precision = defaultdict(list)\n",
        "# average_precisions = defaultdict(list)\n",
        "\n",
        "# # Loop through each fold\n",
        "# for train_index, test_index in cv.split(X_train01, y_train01):\n",
        "#     X_train01_fold, X_test_fold = X_train01.iloc[train_index], X_train01.iloc[test_index]\n",
        "#     y_train01_fold, y_test_fold = y_train01.iloc[train_index], y_train01.iloc[test_index]\n",
        "\n",
        "#     # Loop through classifiers\n",
        "#     for name, clf in classifiers_optimized.items():\n",
        "#         clf.fit(X_train01_fold, y_train01_fold)\n",
        "#         y_pred_proba = clf.predict_proba(X_test_fold)[:, 1]\n",
        "\n",
        "#         precision, recall, _ = precision_recall_curve(y_test_fold, y_pred_proba)\n",
        "#         avg_precision = average_precision_score(y_test_fold, y_pred_proba)\n",
        "\n",
        "#         # Interpolate precision at the points of mean_recall\n",
        "#         interp_prec = np.interp(mean_recall, np.flipud(recall), np.flipud(precision))\n",
        "\n",
        "#         interp_precision[name].append(interp_prec)\n",
        "#         average_precisions[name].append(avg_precision)\n",
        "\n",
        "# # Plot the Precision-Recall curve\n",
        "# for name, precisions in interp_precision.items():\n",
        "#     color = colors.get(name, 'aqua')  # Default color if not found\n",
        "#     mean_precision = np.mean(precisions, axis=0)\n",
        "#     mean_avg_precision = np.mean(average_precisions[name])\n",
        "\n",
        "#     plt.plot(mean_recall, mean_precision, color=color, label=f\"{name}: {mean_avg_precision:.2f}\")\n",
        "\n",
        "# # Set plot options\n",
        "# plt.xlim([0.0, 1.02])\n",
        "# plt.ylim([0.0, 1.02])\n",
        "# plt.xlabel('Recall')\n",
        "# plt.ylabel('Precision')\n",
        "# plt.title('Precision-Recall curve with k-Fold Cross-Validation')\n",
        "\n",
        "# # Hide the right and top spines\n",
        "# plt.gca().spines['right'].set_visible(False)\n",
        "# plt.gca().spines['top'].set_visible(False)\n",
        "\n",
        "# plt.legend(loc=\"upper right\", frameon=False, title='Average Precision', title_fontsize=11)\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "AZ5Ju20RNWmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 타겟 01, 컬럼 CA19-9 단독"
      ],
      "metadata": {
        "id": "4if6Ww_xJlB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train01_CA19 = pd.DataFrame(X_train01['CA19-9'], columns=['CA19-9'])\n",
        "X_train01_CA19"
      ],
      "metadata": {
        "id": "eDvS-_bxJpkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "colors = {\n",
        "    'XGBoost': '#3232FF',\n",
        "    'SVM': '#5050FF',\n",
        "    'K-NN': '#1E82FF',\n",
        "    'LightGBM': '#E6A055',\n",
        "    'Logistic Regression': '#EF904C',\n",
        "    'Random Forest': '#B93232',\n",
        "}\n",
        "\n",
        "# Initialize figure for ROC curve\n",
        "plt.figure(figsize=(6, 6))\n",
        "\n",
        "# Number of splits\n",
        "n_splits = 3\n",
        "cv = StratifiedKFold(n_splits=n_splits)\n",
        "\n",
        "# Initialize dictionaries to store FPR, TPR, and AUC values for each classifier\n",
        "roc_curves = defaultdict(list)\n",
        "roc_aucs = defaultdict(list)\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "# Loop through each fold\n",
        "for train_index, test_index in cv.split(X_train01_CA19, y_train01):\n",
        "    X_train01_fold, X_test_fold = X_train01_CA19.iloc[train_index], X_train01_CA19.iloc[test_index]\n",
        "    y_train01_fold, y_test_fold = y_train01.iloc[train_index], y_train01.iloc[test_index]\n",
        "\n",
        "    # Loop through classifiers\n",
        "    for name, clf in classifiers_optimized.items():\n",
        "        clf.fit(X_train01_fold, y_train01_fold)\n",
        "        y_pred_proba = clf.predict_proba(X_test_fold)[:, 1]\n",
        "\n",
        "        fpr, tpr, _ = roc_curve(y_test_fold, y_pred_proba)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        # Interpolate the TPR to be the same length as mean_fpr\n",
        "        interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
        "        interp_tpr[0] = 0.0\n",
        "\n",
        "        roc_curves[name].append(interp_tpr)\n",
        "        roc_aucs[name].append(roc_auc)\n",
        "\n",
        "for name, tprs in roc_curves.items():\n",
        "    color = colors.get(name, 'aqua')  # Default color if not found\n",
        "    mean_tpr = np.mean(tprs, axis=0)\n",
        "    mean_tpr[-1] = 1.0\n",
        "    mean_auc = auc(mean_fpr, mean_tpr)\n",
        "    std_auc = np.std(roc_aucs[name])\n",
        "    std_tpr = np.std(tprs, axis=0)\n",
        "\n",
        "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "\n",
        "    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color=color, alpha=.2)\n",
        "    plt.plot(mean_fpr, mean_tpr, color=color, label=f\"{name}: {mean_auc:.2f} +/- {std_auc:.2f}\")\n",
        "\n",
        "\n",
        "# Plot random chance line\n",
        "# plt.plot([0, 1], [0, 1], 'k--', label=\"Chance (AUC = 0.5)\")\n",
        "\n",
        "# Set plot options\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.02])\n",
        "plt.xlabel('1 - Specificity (False Positive Rate)')\n",
        "plt.ylabel('Sensitivity (True Positive Rate)')\n",
        "# plt.title('Receiver Operating Characteristic with k-Fold Cross-Validation')\n",
        "\n",
        "# Hide the right and top spines\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "\n",
        "plt.legend(loc=\"lower right\", frameon=False, title='AUC', title_fontsize=11)\n",
        "plt.savefig(base_path + 'ROC2.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UUrw8aCoJ2G9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "colors = {\n",
        "    'XGBoost': '#3232FF',\n",
        "    'SVM': '#5050FF',\n",
        "    'K-NN': '#1E82FF',\n",
        "    'LightGBM': '#E6A055',\n",
        "    'Logistic Regression': '#EF904C',\n",
        "    'Random Forest': '#B93232',\n",
        "}\n",
        "\n",
        "# Initialize figure for ROC curve\n",
        "plt.figure(figsize=(6, 6))\n",
        "\n",
        "# 초기 통합 Confusion Matrix 설정\n",
        "total_cm = np.zeros((2, 2))  # 이진 분류의 경우 2x2 행렬\n",
        "\n",
        "# Confusion Matrix를 저장할 딕셔너리\n",
        "cm_dict = {}\n",
        "cms = []\n",
        "\n",
        "\n",
        "# Number of splits\n",
        "n_splits = 3\n",
        "cv = StratifiedKFold(n_splits=n_splits)\n",
        "\n",
        "# Initialize dictionaries to store FPR, TPR, and AUC values for each classifier\n",
        "roc_curves = defaultdict(list)\n",
        "roc_aucs = defaultdict(list)\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "# Loop through each fold\n",
        "for train_index, test_index in cv.split(X_train01_CA19, y_train01):\n",
        "    X_train01_fold, X_test_fold = X_train01_CA19.iloc[train_index], X_train01_CA19.iloc[test_index]\n",
        "    y_train01_fold, y_test_fold = y_train01.iloc[train_index], y_train01.iloc[test_index]\n",
        "\n",
        "    # Loop through classifiers\n",
        "    for name, clf in classifiers_optimized.items():\n",
        "        clf.fit(X_train01_fold, y_train01_fold)\n",
        "        y_pred_proba = clf.predict_proba(X_test_fold)[:, 1]\n",
        "        y_pred = clf.predict(X_test_fold)\n",
        "\n",
        "        fpr, tpr, _ = roc_curve(y_test_fold, y_pred_proba)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        # Confusion Matrix 계산\n",
        "        cm = confusion_matrix(y_test_fold, y_pred)  # 실제 레이블과 예측 레이블을 사용\n",
        "\n",
        "        # Confusion Matrix를 리스트에 저장\n",
        "        cms.append(cm)\n",
        "\n",
        "        # 통합 Confusion Matrix에 더하기\n",
        "        total_cm += cm\n",
        "\n",
        "        # Interpolate the TPR to be the same length as mean_fpr\n",
        "        interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
        "        interp_tpr[0] = 0.0\n",
        "\n",
        "        roc_curves[name].append(interp_tpr)\n",
        "        roc_aucs[name].append(roc_auc)\n",
        "\n",
        "for name, tprs in roc_curves.items():\n",
        "    color = colors.get(name, 'aqua')  # Default color if not found\n",
        "    mean_tpr = np.mean(tprs, axis=0)\n",
        "    mean_tpr[-1] = 1.0\n",
        "    mean_auc = auc(mean_fpr, mean_tpr)\n",
        "    std_auc = np.std(roc_aucs[name])\n",
        "    std_tpr = np.std(tprs, axis=0)\n",
        "\n",
        "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "\n",
        "    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color=color, alpha=.2)\n",
        "    plt.plot(mean_fpr, mean_tpr, color=color, label=f\"{name}: {mean_auc:.2f} +/- {std_auc:.2f}\")\n",
        "\n",
        "\n",
        "# Plot random chance line\n",
        "# plt.plot([0, 1], [0, 1], 'k--', label=\"Chance (AUC = 0.5)\")\n",
        "\n",
        "# 각 fold에서의 Confusion Matrix를 numpy array로 변환\n",
        "cms = np.array(cms)\n",
        "\n",
        "# 열별로 평균과 표준편차 계산\n",
        "mean_cms = np.mean(cms, axis=0)\n",
        "std_cms = np.std(cms, axis=0)\n",
        "\n",
        "# 열별로 퍼센트와 변화율 계산\n",
        "total_samples_per_class = np.sum(mean_cms, axis=0)\n",
        "mean_percentage = (mean_cms / total_samples_per_class) * 100\n",
        "std_percentage = (std_cms / total_samples_per_class) * 10\n",
        "\n",
        "# Manually create the annotations using list comprehension\n",
        "rows, cols = mean_percentage.shape\n",
        "annotations = [[f\"{mean_percentage[i, j]:.0f}±{std_percentage[i, j]:.0f}%\" for j in range(cols)] for i in range(rows)]\n",
        "\n",
        "# 통합 Confusion Matrix를 그립니다.\n",
        "plt.figure(figsize=(6, 6))\n",
        "\n",
        "sns.heatmap(mean_percentage.T, annot=np.array(annotations).T, fmt='', cmap='coolwarm', xticklabels=['More severe', 'Less severe'], yticklabels=['More severe', 'Less severe'], annot_kws={\"fontsize\": 14}, cbar=False)\n",
        "plt.xlabel('True')\n",
        "plt.ylabel('Predicted')\n",
        "# plt.title('Total Confusion Matrix with Change Rate')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Set plot options\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.02])\n",
        "plt.xlabel('1 - Specificity (False Positive Rate)')\n",
        "plt.ylabel('Sensitivity (True Positive Rate)')\n",
        "# plt.title('Receiver Operating Characteristic with k-Fold Cross-Validation')\n",
        "\n",
        "# Hide the right and top spines\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "\n",
        "plt.legend(loc=\"lower right\", frameon=False, title='AUC', title_fontsize=11)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nlkkQkrtkev3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize figure for PR curve\n",
        "plt.figure(figsize=(6, 6))\n",
        "\n",
        "# Number of splits\n",
        "n_splits = 3\n",
        "cv = StratifiedKFold(n_splits=n_splits)\n",
        "\n",
        "# Initialize dictionaries to store FPR, TPR, and AUC values for each classifier\n",
        "pr_curves = defaultdict(list)\n",
        "pr_aucs = defaultdict(list)\n",
        "mean_recall = np.linspace(0, 1, 100)\n",
        "\n",
        "# Loop through each fold\n",
        "for train_index, test_index in cv.split(X_train01_CA19, y_train01):\n",
        "    X_train01_fold, X_test_fold = X_train01_CA19.iloc[train_index], X_train01_CA19.iloc[test_index]\n",
        "    y_train01_fold, y_test_fold = y_train01.iloc[train_index], y_train01.iloc[test_index]\n",
        "\n",
        "    # Loop through classifiers\n",
        "    for name, clf in classifiers_optimized.items():\n",
        "        clf.fit(X_train01_fold, y_train01_fold)\n",
        "        y_pred_proba = clf.predict_proba(X_test_fold)[:, 1]\n",
        "\n",
        "        precision, recall, _ = precision_recall_curve(y_test_fold, y_pred_proba)\n",
        "        avg_precision = average_precision_score(y_test_fold, y_pred_proba)\n",
        "\n",
        "        # Interpolate the precision to be the same length as mean_recall\n",
        "        interp_precision = np.interp(mean_recall, recall[::-1], precision[::-1])\n",
        "\n",
        "        pr_curves[name].append(interp_precision)\n",
        "        pr_aucs[name].append(avg_precision)\n",
        "\n",
        "# Plot average PR curves\n",
        "for name, precisions in pr_curves.items():\n",
        "    color = colors.get(name, 'aqua')  # Default color if not found\n",
        "    mean_precision = np.mean(precisions, axis=0)\n",
        "    std_precision = np.std(precisions, axis=0)\n",
        "    mean_auc = np.mean(pr_aucs[name])\n",
        "    std_auc = np.std(pr_aucs[name])\n",
        "\n",
        "    precision_upper = np.minimum(mean_precision + std_precision, 1)\n",
        "    precision_lower = np.maximum(mean_precision - std_precision, 0)\n",
        "\n",
        "    plt.fill_between(mean_recall, precision_lower, precision_upper, color=color, alpha=.2)\n",
        "    plt.plot(mean_recall, mean_precision, color=color,\n",
        "             label=f\"{name}: {mean_auc:.2f} +/- {std_auc:.2f}\")\n",
        "\n",
        "\n",
        "# Set plot options\n",
        "plt.xlim([0.0, 1.05])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "\n",
        "# Hide the right and top spines\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "\n",
        "plt.legend(loc=\"lower right\", frameon=False, title='AUC', title_fontsize=11)\n",
        "plt.savefig(base_path + 'ROC_Recall_2.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1bsk6YGnvzw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9TUzjBgXxA_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 012, 컬럼 3개"
      ],
      "metadata": {
        "id": "t9zh3VGaKfOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_t.drop('Stage(TNM)', axis=1)\n",
        "y = df_t['Stage(TNM)']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) # 랜덤 스테이트 42?\n",
        "\n",
        "if is_dropcol:\n",
        "  X = df_t_drop.drop('Stage(TNM)', axis=1)\n",
        "  y = df_t_drop['Stage(TNM)']\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) # 랜덤 스테이트 42?"
      ],
      "metadata": {
        "id": "JhNIzfw19H_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import StratifiedKFold\n",
        "# from collections import defaultdict\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# colors = {\n",
        "#     'XGBoost': '#3232FF',\n",
        "#     'SVM': '#5050FF',\n",
        "#     'K-NN': '#1E82FF',\n",
        "#     'LightGBM': '#E6A055',\n",
        "#     'Logistic Regression': '#EF904C',\n",
        "#     'Random Forest': '#B93232',\n",
        "# }\n",
        "\n",
        "# # Initialize figure for ROC curve\n",
        "# plt.figure(figsize=(6, 6))\n",
        "\n",
        "# # Number of splits\n",
        "# n_splits = 3\n",
        "# cv = StratifiedKFold(n_splits=n_splits)\n",
        "\n",
        "# # Initialize dictionaries to store FPR, TPR, and AUC values for each classifier\n",
        "# roc_curves = defaultdict(list)\n",
        "# roc_aucs = defaultdict(list)\n",
        "# mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "# # ... (이전 코드와 동일)\n",
        "\n",
        "# # Number of classes in the dataset\n",
        "# num_classes = 3  # Replace with your actual number of classes\n",
        "\n",
        "# # Initialize dictionaries to store interpolated TPR for each classifier and class\n",
        "# interp_tprs = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "# # Loop through each fold\n",
        "# for train_index, test_index in cv.split(X_train, y_train):\n",
        "#     X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
        "#     y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
        "\n",
        "#     # Binarize the labels for multiclass ROC\n",
        "#     y_test_fold_bin = label_binarize(y_test_fold, classes=[0, 1, 2])  # classes should be your actual classes\n",
        "\n",
        "#     # Loop through classifiers\n",
        "#     for name, clf in classifiers_optimized.items():\n",
        "#         clf.fit(X_train_fold, y_train_fold)\n",
        "#         y_pred_proba = clf.predict_proba(X_test_fold)\n",
        "\n",
        "#         # Compute ROC curve and ROC area for each class\n",
        "#         fpr = dict()\n",
        "#         tpr = dict()\n",
        "#         roc_auc = dict()\n",
        "\n",
        "#         for i in range(num_classes):\n",
        "#             fpr[i], tpr[i], _ = roc_curve(y_test_fold_bin[:, i], y_pred_proba[:, i])\n",
        "#             roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "#             # Interpolate the TPR to be the same length as mean_fpr\n",
        "#             interp_tpr = np.interp(mean_fpr, fpr[i], tpr[i])\n",
        "#             interp_tpr[0] = 0.0\n",
        "#             interp_tprs[name][i].append(interp_tpr)\n",
        "\n",
        "#         roc_curves[name].append(interp_tpr)\n",
        "#         roc_aucs[name].append(roc_auc)\n",
        "\n",
        "# for name, tprs in roc_curves.items():\n",
        "#     color = colors.get(name, 'aqua')  # Default color if not found\n",
        "#     mean_tpr = np.mean(tprs, axis=0)\n",
        "#     mean_tpr[-1] = 1.0\n",
        "#     mean_auc = auc(mean_fpr, mean_tpr)\n",
        "#     std_auc = np.std(roc_aucs[name])\n",
        "#     std_tpr = np.std(tprs, axis=0)\n",
        "\n",
        "#     tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "#     tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "\n",
        "#     plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color=color, alpha=.2)\n",
        "#     plt.plot(mean_fpr, mean_tpr, color=color, label=f\"{name}: {mean_auc:.2f} +/- {std_auc:.2f}\")\n",
        "\n",
        "\n",
        "# # Plot random chance line\n",
        "# # plt.plot([0, 1], [0, 1], 'k--', label=\"Chance (AUC = 0.5)\")\n",
        "\n",
        "# # Set plot options\n",
        "# plt.xlim([0.0, 1.0])\n",
        "# plt.ylim([0.0, 1.02])\n",
        "# plt.xlabel('1 - Specificity (False Positive Rate)')\n",
        "# plt.ylabel('Sensitivity (True Positive Rate)')\n",
        "# # plt.title('Receiver Operating Characteristic with k-Fold Cross-Validation')\n",
        "\n",
        "# # Hide the right and top spines\n",
        "# plt.gca().spines['right'].set_visible(False)\n",
        "# plt.gca().spines['top'].set_visible(False)\n",
        "\n",
        "# plt.legend(loc=\"lower right\", frameon=False, title='AUC', title_fontsize=11)\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "NX0TRTvR9Qxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# feature importance"
      ],
      "metadata": {
        "id": "-OnopbuMxEjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feature selection via Feature Importance\n",
        "m = 20\n",
        "\n",
        "X01_feature = df_t01.drop(columns=[\"Stage(TNM)\"])\n",
        "y01_feature = df_t01['Stage(TNM)']\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X01_feature, y01_feature)\n",
        "print(\"Train ACC : %.4f\" % accuracy_score(y01_feature, rf.predict(X01_feature)))\n",
        "fi_df = pd.DataFrame({'feature':X01_feature.columns, 'importance':rf.feature_importances_})\n",
        "selected_cols = fi_df.sort_values(by=\"importance\", ascending=False)[:m][\"feature\"].values\n",
        "\n",
        "display(selected_cols)\n",
        "\n",
        "X01_feature = df_t[selected_cols]\n",
        "display(X01_feature)"
      ],
      "metadata": {
        "id": "PBuxh7NDxGa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# fi_df를 feature importance가 높은 순서대로 정렬합니다.\n",
        "sorted_fi_df = fi_df.sort_values(by=\"importance\", ascending=False)\n",
        "\n",
        "# 그래프를 그리기 위해 데이터를 준비합니다.\n",
        "features = sorted_fi_df['feature']\n",
        "importances = sorted_fi_df['importance']\n",
        "\n",
        "# 그래프를 그립니다.\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(features, importances)\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Importance')\n",
        "plt.title('Feature Importance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uRPzG37AzAnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# feature selection via Feature Importance\n",
        "X012_feature = df_t.drop(columns=[\"Stage(TNM)\"])\n",
        "y012_feature = df_t['Stage(TNM)']\n",
        "\n",
        "# 각 feature와 target variable 사이의 correlation을 계산합니다.\n",
        "correlations = X012_feature.corrwith(y012_feature).sort_values(ascending=False)\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X012_feature, y012_feature)\n",
        "print(\"Train ACC : %.4f\" % accuracy_score(y012_feature, rf.predict(X012_feature)))\n",
        "fi_df = pd.DataFrame({'feature':X012_feature.columns, 'importance':rf.feature_importances_})\n",
        "selected_cols = fi_df.sort_values(by=\"importance\", ascending=False)[:m][\"feature\"].values\n",
        "\n",
        "# Feature-Target Correlation 그래프를 그릴 때 selected_cols 순서에 따라 정렬\n",
        "sorted_correlations = correlations.loc[selected_cols]\n",
        "\n",
        "display(selected_cols)\n",
        "\n",
        "X012_feature = df_t[selected_cols]\n",
        "display(X012_feature)\n",
        "\n",
        "# fi_df를 feature importance가 높은 순서대로 정렬합니다.\n",
        "sorted_fi_df = fi_df.sort_values(by=\"importance\", ascending=False)\n",
        "\n",
        "# 그래프를 그리기 위해 데이터를 준비합니다.\n",
        "features = sorted_fi_df['feature']\n",
        "importances = sorted_fi_df['importance']"
      ],
      "metadata": {
        "id": "_Jo1peN-zHuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Importance 그래프\n",
        "plt.figure(figsize=(9, 2.8))\n",
        "plt.bar(features[:20], importances[:20], color='#778899')\n",
        "plt.ylabel('Importance')\n",
        "plt.xticks([])\n",
        "plt.title('Feature Importance')\n",
        "plt.savefig(base_path + 'Feature importance.png')\n",
        "plt.show()\n",
        "\n",
        "# Feature-Target Correlation 그래프\n",
        "plt.figure(figsize=(9, 2.8))\n",
        "plt.bar(sorted_correlations.index, sorted_correlations.values, color='#778899')\n",
        "plt.xticks(rotation=45)\n",
        "# plt.xlabel('Features')\n",
        "plt.ylabel('Correlation')\n",
        "plt.title('Feature Correlation')\n",
        "plt.savefig(base_path + 'Feature correlation.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4Mrh5LVr1wQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Figure4."
      ],
      "metadata": {
        "id": "8tk4J5qD5aPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_vc"
      ],
      "metadata": {
        "id": "oxQKM0D75bSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_vd"
      ],
      "metadata": {
        "id": "aPIcc5Vh6DSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mfmc4YXW6FaH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}